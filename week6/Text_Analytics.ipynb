{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Text_Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week6/Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 6\n",
        "# Text Analytics\n",
        "\n",
        "[Text Analytics](https://people.ischool.berkeley.edu/~hearst/text-mining.html) (or text mining) is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles.\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. Summary\n",
        "* 1.1 Applications\n",
        "* 1.2 Tokenization and Stopwords\n",
        "* 1.3 Stemming and Lemmatization\n",
        "* 1.4 Text Representation\n",
        "\n",
        "#### 2. Text Preparation\n",
        "* 2.1 Install spaCy\n",
        "* 2.2 Tokenization\n",
        "* 2.3 Dependency Parsing\n",
        "* 2.4 Remove Stopwords\n",
        "* 2.5 Lemmatization\n",
        "* 2.6 Entity Detection \n",
        "\n",
        "#### 3. Text Representation\n",
        "* 3.1 Bag of Words (BOW)\n",
        "* 3.2 TF-IDF Representation\n",
        "\n",
        "#### 4. Text Classification: Alexa Reviews\n",
        "* 4.1 Load and prepare data\n",
        "* 4.2 Classification of the reviews using logistic regression\n",
        "* 4.3 BONUS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0VsVUPLT4Tn"
      },
      "source": [
        "## 1. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvrJj5yT8uQ"
      },
      "source": [
        "### 1.1 Applications\n",
        "There are many applications of text analytics, for example:\n",
        "* Search for relevant websites or articles using a search engine\n",
        "* Sentiment Analysis (e.g. classify tweets or film reviews as positive, neutral or negative)\n",
        "* Chatbots (e.g. Siri, Alexa)\n",
        "* Project idea: The Impact of Donald Trump’s Tweets on Financial Market\n",
        "* Etc.\n",
        "\n",
        "### 1.2 Tokenization and Stopwords\n",
        "Tokens are the elementary building blocks (words, numbers, characters) in a document. Tokenization is the process of splitting an input\n",
        "sequence into tokens. Example: \"I love data science\" --> \"I\", \"love\", \"data\", \"science\". Stopwords are common words that appear very frequently (e.g. \"is\", \"and\", \"you\", etc.). It is convenient to remove them as they do not add much to the content of a document and are therefore generally not useful for text analysis or, worse still, make it worse.\n",
        "\n",
        "### 1.3 Lemmatization and Stemming\n",
        "* Goal: have the same token for different forms of a word (e.g. fishing, fished, fisher, fishers, etc.)\n",
        "* Lemmatization: Find what is the lemma of a word (e.g. feet -> foot)\n",
        "* Stemming: one method for lemmatization where rules that remove the ending of a word are applied (e.g. fishing -> fish)\n",
        "\n",
        "\n",
        "### 1.4 Text Representation\n",
        "* Goal: transform text such that it can be used for text analysis\n",
        "* Bag of Words (BOW): works in many case but order is not preserved (solution: n-grams)\n",
        "* TF-IDF: emphasizes important words"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXlVy05IUC-D"
      },
      "source": [
        "## 2. Text Preparation\n",
        "In this section, we explain how to prepare a text for analysis. This includes tockeninzing the text, removing stopwords, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA4tt9roTp8T"
      },
      "source": [
        "### 2.1 Install spaCy\n",
        "[spaCy](https://spacy.io/) is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
        "\n",
        "We install the library and its English-language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf6I0JIYTp8U",
        "outputId": "6bea4c14-41e0-4e96-b228-cedabbd9b31b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Load the english language model\n",
        "!python -m spacy download en"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.1)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOeeRlqTp8Z"
      },
      "source": [
        "# Import required packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUibE-SyTp8d"
      },
      "source": [
        "### 2.2 Tokenization\n",
        "\n",
        "Tokenization is the process of breaking a text into pieces called tokens. A token simply refers to an individual part of a sentence having some semantic value. SpaCy‘s tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
        "\n",
        "Let’s take a look at a simple example. Imagine we have the following text, and we would like to tokenize it:\n",
        "\n",
        "> When learning data science, you shouldn't get discouraged!\n",
        "\n",
        "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
        "\n",
        "There are a couple of different ways we can appoach this. The first is called __word tokenization__, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9OfB1BTp8e",
        "outputId": "c192739f-c869-4e79-a2bd-afd9ce8eeb78",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Declare the text\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "# spaCy object is used to create a document\n",
        "my_doc = sp(text)\n",
        "\n",
        "my_doc"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "When learning data science, you shouldn't get discouraged!\n",
              "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7fwTbPZdXJ",
        "outputId": "5c55369e-3904-47e4-c08f-a2042a66a4a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "type(my_doc)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7-T-BkYXZM",
        "outputId": "ca0fa4e7-af22-4a18-e36a-011b27988fbb",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Create list of word tokens\n",
        "token_list = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "token_list"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When',\n",
              " 'learning',\n",
              " 'data',\n",
              " 'science',\n",
              " ',',\n",
              " 'you',\n",
              " 'should',\n",
              " \"n't\",\n",
              " 'get',\n",
              " 'discouraged',\n",
              " '!',\n",
              " '\\n',\n",
              " 'Challenges',\n",
              " 'and',\n",
              " 'setbacks',\n",
              " 'are',\n",
              " \"n't\",\n",
              " 'failures',\n",
              " ',',\n",
              " 'they',\n",
              " \"'re\",\n",
              " 'just',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'journey',\n",
              " '.',\n",
              " 'You',\n",
              " \"'ve\",\n",
              " 'got',\n",
              " 'this',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi29AcSsTp8j"
      },
      "source": [
        "\n",
        "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and has thus broken them down into two distinct tokens.\n",
        "\n",
        "In the example above, we first load the language dictionary. Here we load the english one using `spacy.load('en_core_web_sm')` create an object of this class, \"sp\", which is used to create documents with linguistic annotations and various language properties. After creating the document, we create a list of tokens.\n",
        "\n",
        "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute, as shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvkGwDJoWoIs",
        "outputId": "3b26201d-1154-457f-90ca-2d3be4cb98f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# POS\n",
        "for word in my_doc:\n",
        "    print(word.text, word.pos_)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When ADV\n",
            "learning VERB\n",
            "data NOUN\n",
            "science NOUN\n",
            ", PUNCT\n",
            "you PRON\n",
            "should VERB\n",
            "n't PART\n",
            "get AUX\n",
            "discouraged ADJ\n",
            "! PUNCT\n",
            "\n",
            " SPACE\n",
            "Challenges NOUN\n",
            "and CCONJ\n",
            "setbacks NOUN\n",
            "are AUX\n",
            "n't PART\n",
            "failures NOUN\n",
            ", PUNCT\n",
            "they PRON\n",
            "'re AUX\n",
            "just ADV\n",
            "part NOUN\n",
            "of ADP\n",
            "the DET\n",
            "journey NOUN\n",
            ". PUNCT\n",
            "You PRON\n",
            "'ve AUX\n",
            "got VERB\n",
            "this DET\n",
            "! PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUsKHgp7Y3yM",
        "outputId": "ba72c960-cd43-433f-8058-e7dc0ef9f898",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Another example\n",
        "doc1 = sp(\"I like to fish\") # verb\n",
        "doc2 = sp(\"I eat a fish\") # noun\n",
        "\n",
        "for word in doc1:\n",
        "  print(word.text, word.pos_)\n",
        "\n",
        "print(\"-----------------\")\n",
        "\n",
        "for word in doc2:\n",
        "  print(word.text, word.pos_)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I PRON\n",
            "like VERB\n",
            "to PART\n",
            "fish VERB\n",
            "-----------------\n",
            "I PRON\n",
            "eat VERB\n",
            "a DET\n",
            "fish NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCiU1-PDXKpp"
      },
      "source": [
        "\n",
        "If we want, we can also break the text into sentences rather than words. This is called __sentence tokenization__. When performing sentence tokenization, the tokenizer looks for specific characters that normally fall between sentences, like periods, exclaimation points, and newline characters."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odi_WkNZTp8k",
        "outputId": "0f801ae4-b9ee-4010-8aef-3bd2aa7a1cf2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "\n",
        "for sent in my_doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "\n",
        "sents_list"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"When learning data science, you shouldn't get discouraged!\\n\",\n",
              " \"Challenges and setbacks aren't failures, they're just part of the journey.\",\n",
              " \"You've got this!\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te403rkXbGKf"
      },
      "source": [
        "### 2.3 Dependency Parsing\n",
        "__Depenency parsing__ is a language processing technique that allows to better determine the meaning of a sentence by analyzing how it is constructed to determine how the individual words relate to each other.\n",
        "\n",
        "Consider, for example, the sentence “Joe throws the ball.” We have two nouns (Joe and ball) and one verb (throws). But we can’t just look at these words individually, or we may end up thinking that the ball throws Joe! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words.\n",
        "\n",
        "Below, we have a short sentence. We’ll use a spaCy method called `noun_chunks`, which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFLButK_bOor",
        "outputId": "2d6a9f77-c286-43fa-dcea-e84c4c778e8b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "doc = sp(\" Joe threw a ball, and President Donald, in pursuit of the ball, hit a wall.\") # notice the space at the beginning\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Joe Joe nsubj threw\n",
            "a ball ball dobj threw\n",
            "President Donald Donald nsubj hit\n",
            "pursuit pursuit pobj in\n",
            "the ball ball pobj of\n",
            "a wall wall dobj hit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VmqTQQ9bYTl",
        "outputId": "684568a5-981a-42eb-a8dc-b40c93a31b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Let's visualize this\n",
        "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 120})"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"4ede40ea1d5940fdadaafb02dc3cfb8b-0\" class=\"displacy\" width=\"1970\" height=\"497.0\" direction=\"ltr\" style=\"max-width: none; height: 497.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\"> </tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">Joe</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">threw</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">ball,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">President</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">Donald,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">pursuit</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">ball,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1610\">hit</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1610\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"407.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">wall.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-0\" stroke-width=\"2px\" d=\"M70,362.0 C70,302.0 145.0,302.0 145.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\"></textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,364.0 L62,352.0 78,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-1\" stroke-width=\"2px\" d=\"M190,362.0 C190,302.0 265.0,302.0 265.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M190,364.0 L182,352.0 198,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-2\" stroke-width=\"2px\" d=\"M430,362.0 C430,302.0 505.0,302.0 505.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M430,364.0 L422,352.0 438,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-3\" stroke-width=\"2px\" d=\"M310,362.0 C310,242.0 510.0,242.0 510.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M510.0,364.0 L518.0,352.0 502.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-4\" stroke-width=\"2px\" d=\"M310,362.0 C310,182.0 635.0,182.0 635.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M635.0,364.0 L643.0,352.0 627.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-5\" stroke-width=\"2px\" d=\"M790,362.0 C790,302.0 865.0,302.0 865.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,364.0 L782,352.0 798,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-6\" stroke-width=\"2px\" d=\"M910,362.0 C910,62.0 1605.0,62.0 1605.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M910,364.0 L902,352.0 918,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-7\" stroke-width=\"2px\" d=\"M1030,362.0 C1030,122.0 1600.0,122.0 1600.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1030,364.0 L1022,352.0 1038,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-8\" stroke-width=\"2px\" d=\"M1030,362.0 C1030,302.0 1105.0,302.0 1105.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1105.0,364.0 L1113.0,352.0 1097.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-9\" stroke-width=\"2px\" d=\"M1150,362.0 C1150,302.0 1225.0,302.0 1225.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1225.0,364.0 L1233.0,352.0 1217.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-10\" stroke-width=\"2px\" d=\"M1390,362.0 C1390,302.0 1465.0,302.0 1465.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1390,364.0 L1382,352.0 1398,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-11\" stroke-width=\"2px\" d=\"M1270,362.0 C1270,242.0 1470.0,242.0 1470.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1470.0,364.0 L1478.0,352.0 1462.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-12\" stroke-width=\"2px\" d=\"M310,362.0 C310,2.0 1610.0,2.0 1610.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1610.0,364.0 L1618.0,352.0 1602.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-13\" stroke-width=\"2px\" d=\"M1750,362.0 C1750,302.0 1825.0,302.0 1825.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1750,364.0 L1742,352.0 1758,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-14\" stroke-width=\"2px\" d=\"M1630,362.0 C1630,242.0 1830.0,242.0 1830.0,362.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-4ede40ea1d5940fdadaafb02dc3cfb8b-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1830.0,364.0 L1838.0,352.0 1822.0,352.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Z6MiCtTp8n"
      },
      "source": [
        "### 2.4 Remove Stopwords\n",
        "Most text data that we work with is going to contain a lot of words that are not actually useful to us. These words, called stopwords, are useful in human speech, but they do not have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process). This makes text analysis more efficient.\n",
        "\n",
        "Let’s take a look at the stopwords spaCy includes by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kB8kI0eTp8o",
        "outputId": "610932ba-a190-4c92-a5ee-8724eb9743f9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Import stopwords from English language\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Print total number of stopwords\n",
        "print('Number of stopwords: %d' % len(spacy_stopwords))\n",
        "\n",
        "# Print 20 stopwords\n",
        "print('20 stopwords: %s' % list(spacy_stopwords)[:20])"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of stopwords: 326\n",
            "20 stopwords: ['anywhere', 'above', 'both', 'last', 'otherwise', 'thence', 'does', 'seems', 'with', 'so', 'indeed', 'herself', 'a', 'nobody', 'whereby', 'whither', 'were', 'call', 'except', 'anyone']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0Wfvu4Tp8q"
      },
      "source": [
        "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq5yvoZtlhsd",
        "outputId": "7d7d5019-db87-4e6f-86eb-177306a8346c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Wgich words will be removed?\n",
        "my_doc"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "When learning data science, you shouldn't get discouraged!\n",
              "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54Kiz9zTp8r",
        "outputId": "509b3890-91f0-451d-d2c5-247175f03f59",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Declare list for filtered sentence\n",
        "filtered_sent = []\n",
        "\n",
        "# Filter stopwords\n",
        "for word in my_doc:\n",
        "    if word.is_stop == False:\n",
        "        filtered_sent.append(word)\n",
        "\n",
        "filtered_sent"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[learning,\n",
              " data,\n",
              " science,\n",
              " ,,\n",
              " discouraged,\n",
              " !,\n",
              " ,\n",
              " Challenges,\n",
              " setbacks,\n",
              " failures,\n",
              " ,,\n",
              " journey,\n",
              " .,\n",
              " got,\n",
              " !]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOvM2NFdl29W",
        "outputId": "780ad1d8-c245-42dc-a98b-0429938361a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# We can also remove the punctuation\n",
        "filtered_sent2 = []\n",
        "removed_tokens = []\n",
        "\n",
        "# Filter stopwords, punctuation and spaces\n",
        "for word in my_doc:\n",
        "  if (word.is_stop == True) or (word.is_punct == True) or (word.is_space == True):\n",
        "    removed_tokens.append(word)\n",
        "  else:\n",
        "    filtered_sent2.append(word)\n",
        "\n",
        "removed_tokens"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[When,\n",
              " ,,\n",
              " you,\n",
              " should,\n",
              " n't,\n",
              " get,\n",
              " !,\n",
              " ,\n",
              " and,\n",
              " are,\n",
              " n't,\n",
              " ,,\n",
              " they,\n",
              " 're,\n",
              " just,\n",
              " part,\n",
              " of,\n",
              " the,\n",
              " .,\n",
              " You,\n",
              " 've,\n",
              " this,\n",
              " !]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-ebvZom1EP",
        "outputId": "c6aca708-3e4b-4b22-a455-6c65575923b3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "filtered_sent2"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[learning,\n",
              " data,\n",
              " science,\n",
              " discouraged,\n",
              " Challenges,\n",
              " setbacks,\n",
              " failures,\n",
              " journey,\n",
              " got]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbw_TUkTp8v"
      },
      "source": [
        "### 2.5 Lemmatization\n",
        "Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
        "\n",
        "One method for doing this is called __stemming__. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple stemming is often all that’s needed, but lemmatization—which actually looks at words and their roots (called lemma) as described in the dictionary—is more precise (e.g feet -> foot).\n",
        "\n",
        "Let's look at this simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqSImYfTp8v",
        "outputId": "4c7929ed-733d-4a4b-b651-7f38f60bac9f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Lemmatization\n",
        "lem = sp(\"run runs ran running runner runners\")\n",
        "\n",
        "# Find lemma for each word\n",
        "for word in lem:\n",
        "    print(word.text, word.lemma_)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run run\n",
            "runs run\n",
            "ran run\n",
            "running run\n",
            "runner runner\n",
            "runners runner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7J5Na4DTp84"
      },
      "source": [
        "### 2.6 Entity Detection\n",
        "\n",
        "__Entity detection__, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within a text. This is really helpful for quickly extracting information from the text, since you can quickly pick out important topics or indentify key sections of it.\n",
        "\n",
        "Let’s try out some entity detection using a few paragraphs from this [article](https://www.bloomberg.com/features/trump-tweets-market/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlb9S71Tp84",
        "outputId": "2ecc1a44-c508-40a2-a9a5-206ab4e4d71f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "article = sp(\"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.\n",
        "\n",
        "Before U.S. stocks peaked in late January, he drew a direct connection between the increase in market value of American companies and his administration’s pro-growth policies on more than 10 occasions in that month alone.\n",
        "\"\"\")\n",
        "\n",
        "entities = [(i, i.label_, i.label) for i in article.ents]\n",
        "entities"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Donald Trump, 'PERSON', 380),\n",
              " (Twitter, 'ORG', 383),\n",
              " (American, 'NORP', 381),\n",
              " (U.S., 'GPE', 384),\n",
              " (late January, 'DATE', 391),\n",
              " (American, 'NORP', 381),\n",
              " (more than 10, 'CARDINAL', 397),\n",
              " (that month alone, 'DATE', 391)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ScYkq7Tp87"
      },
      "source": [
        "The above example how spaCy is able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n",
        "\n",
        "Using `displaCy` we can also visualize the text, with each identified entity highlighted by a color and labeled. We’ll use `style = \"ent\"` to tell displaCy that we want to visualize entities here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGDOBT6zTp88",
        "outputId": "30d906d3-3e42-4a04-90c3-6026b7735e13",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "displacy.render(article, style = \"ent\", jupyter = True)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>President \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Donald Trump\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " gets a lot of attention for using \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Twitter\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " to attack \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.</br></br>Before \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.S.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " stocks peaked in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    late January\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he drew a direct connection between the increase in market value of \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " companies and his administration’s pro-growth policies on \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    more than 10\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " occasions in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    that month alone\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZOutDwsYij"
      },
      "source": [
        "## 3. Text Representation\n",
        "We now show how to transform a text into an usable input for text classification. We use the first sentence of the article from the last section and two other sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MjeqoQxdOx"
      },
      "source": [
        "# Article as a string, not a spacy object\n",
        "article = \"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.\"\"\"\n",
        "\n",
        "# Sentences\n",
        "s1 = \"\"\"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\"\"\" # from Shaquille O'Neal\n",
        "s2 = \"\"\"Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.\"\"\" # from Mitt Romney\n",
        "\n",
        "# List of sentences\n",
        "texts = [article, s1, s2]"
      ],
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCFQrtHLtMTt"
      },
      "source": [
        "### 3.1 Bag of Words (BOW)\n",
        "We use the [CountVectorizer](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html) class of scikit learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyzBm0BUtLc0",
        "outputId": "77409b9e-ab8d-448b-98f9-97b1f2966e98",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Using default tokenizer \n",
        "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
        "bow = count.fit_transform(texts)\n",
        "\n",
        "# Show feature matrix\n",
        "bow.toarray()"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 0,\n",
              "        1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1,\n",
              "        0, 0, 0, 1, 1, 0, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0,\n",
              "        1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 2, 0,\n",
              "        0, 1, 1, 0, 0, 1, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmUUc0kzi3_",
        "outputId": "37e2d958-ecc0-4ddb-946d-a54d667c5299",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['american',\n",
              " 'american trading',\n",
              " 'attack',\n",
              " 'attack american',\n",
              " 'attention',\n",
              " 'attention using',\n",
              " 'companies',\n",
              " 'degree',\n",
              " 'degree trump',\n",
              " 'donald',\n",
              " 'donald trump',\n",
              " 'foes',\n",
              " 'foes media',\n",
              " 'fraud',\n",
              " 'fraud promises',\n",
              " 'friend',\n",
              " 'friend picassos',\n",
              " 'gets',\n",
              " 'gets lot',\n",
              " 'great',\n",
              " 'great friend',\n",
              " 'look',\n",
              " 'lot',\n",
              " 'lot attention',\n",
              " 'media',\n",
              " 'media companies',\n",
              " 'partners',\n",
              " 'partners political',\n",
              " 'phony',\n",
              " 'phony fraud',\n",
              " 'picassos',\n",
              " 'picassos plane',\n",
              " 'plane',\n",
              " 'plane look',\n",
              " 'political',\n",
              " 'political foes',\n",
              " 'president',\n",
              " 'president donald',\n",
              " 'promises',\n",
              " 'promises worthless',\n",
              " 'trading',\n",
              " 'trading partners',\n",
              " 'trump',\n",
              " 'trump gets',\n",
              " 'trump great',\n",
              " 'trump phony',\n",
              " 'trump university',\n",
              " 'twitter',\n",
              " 'twitter attack',\n",
              " 'university',\n",
              " 'using',\n",
              " 'using twitter',\n",
              " 'worthless',\n",
              " 'worthless degree']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCSvGUTz2tl",
        "outputId": "3791858d-c230-4022-8093-8046b23602f6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Show as a dataframe\n",
        "pd.DataFrame(\n",
        "    bow.todense(), \n",
        "    columns=feature_names\n",
        "    )"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>american</th>\n",
              "      <th>american trading</th>\n",
              "      <th>attack</th>\n",
              "      <th>attack american</th>\n",
              "      <th>attention</th>\n",
              "      <th>attention using</th>\n",
              "      <th>companies</th>\n",
              "      <th>degree</th>\n",
              "      <th>degree trump</th>\n",
              "      <th>donald</th>\n",
              "      <th>donald trump</th>\n",
              "      <th>foes</th>\n",
              "      <th>foes media</th>\n",
              "      <th>fraud</th>\n",
              "      <th>fraud promises</th>\n",
              "      <th>friend</th>\n",
              "      <th>friend picassos</th>\n",
              "      <th>gets</th>\n",
              "      <th>gets lot</th>\n",
              "      <th>great</th>\n",
              "      <th>great friend</th>\n",
              "      <th>look</th>\n",
              "      <th>lot</th>\n",
              "      <th>lot attention</th>\n",
              "      <th>media</th>\n",
              "      <th>media companies</th>\n",
              "      <th>partners</th>\n",
              "      <th>partners political</th>\n",
              "      <th>phony</th>\n",
              "      <th>phony fraud</th>\n",
              "      <th>picassos</th>\n",
              "      <th>picassos plane</th>\n",
              "      <th>plane</th>\n",
              "      <th>plane look</th>\n",
              "      <th>political</th>\n",
              "      <th>political foes</th>\n",
              "      <th>president</th>\n",
              "      <th>president donald</th>\n",
              "      <th>promises</th>\n",
              "      <th>promises worthless</th>\n",
              "      <th>trading</th>\n",
              "      <th>trading partners</th>\n",
              "      <th>trump</th>\n",
              "      <th>trump gets</th>\n",
              "      <th>trump great</th>\n",
              "      <th>trump phony</th>\n",
              "      <th>trump university</th>\n",
              "      <th>twitter</th>\n",
              "      <th>twitter attack</th>\n",
              "      <th>university</th>\n",
              "      <th>using</th>\n",
              "      <th>using twitter</th>\n",
              "      <th>worthless</th>\n",
              "      <th>worthless degree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   american  american trading  ...  worthless  worthless degree\n",
              "0         1                 1  ...          0                 0\n",
              "1         0                 0  ...          0                 0\n",
              "2         0                 0  ...          1                 1\n",
              "\n",
              "[3 rows x 54 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tNGrTutQ2r"
      },
      "source": [
        "### 3.2 TF-IDF Representation\n",
        "\n",
        "\n",
        "Recall that:\n",
        "\n",
        "- term frequency tf = count(word, document) / len(document) \n",
        "- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n",
        "- tf-idf = tf * idf \n",
        "\n",
        "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n",
        "\n",
        "**Note**: In the example below, you may not get the exact values by multiplying those two numbers, because nltk normalizes each row to have norm of 1. However the relative importance of the terms won't change.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djkUI6hXtWlr",
        "outputId": "1d614ed1-897e-445e-c878-01fdb6a631dd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "# Using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>american</th>\n",
              "      <th>attack</th>\n",
              "      <th>attention</th>\n",
              "      <th>companies</th>\n",
              "      <th>degree</th>\n",
              "      <th>donald</th>\n",
              "      <th>foes</th>\n",
              "      <th>fraud</th>\n",
              "      <th>friend</th>\n",
              "      <th>gets</th>\n",
              "      <th>great</th>\n",
              "      <th>look</th>\n",
              "      <th>lot</th>\n",
              "      <th>media</th>\n",
              "      <th>partners</th>\n",
              "      <th>phony</th>\n",
              "      <th>picassos</th>\n",
              "      <th>plane</th>\n",
              "      <th>political</th>\n",
              "      <th>president</th>\n",
              "      <th>promises</th>\n",
              "      <th>trading</th>\n",
              "      <th>trump</th>\n",
              "      <th>twitter</th>\n",
              "      <th>university</th>\n",
              "      <th>using</th>\n",
              "      <th>worthless</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.154057</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.154057</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.260841</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.247433</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.247433</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.212236</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.424472</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.359347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   american    attack  attention  ...  university     using  worthless\n",
              "0  0.260841  0.260841   0.260841  ...    0.000000  0.260841   0.000000\n",
              "1  0.000000  0.000000   0.000000  ...    0.000000  0.000000   0.000000\n",
              "2  0.000000  0.000000   0.000000  ...    0.359347  0.000000   0.359347\n",
              "\n",
              "[3 rows x 27 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AcyNjGlfHjlo",
        "outputId": "e0984020-dbf5-4121-f99c-069656211b94",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "texts"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['\\nPresident Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies.',\n",
              " \"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\",\n",
              " 'Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARu2ONJb06Sr"
      },
      "source": [
        "## 4. Text Classification: Alexa reviews\n",
        "\n",
        "We are going to use a real-world data set: [Amazon Alexa product reviews](https://www.kaggle.com/sid321axn/amazon-alexa-reviews/download).\n",
        "\n",
        "This data set comes as a tab-separated file (.tsv). It has has five columns: `rating`, `date`, `variation`, `verified_reviews`, `feedback`.\n",
        "\n",
        "`rating` denotes the rating each user gave Alexa (out of 5). `date` indicates the date of the review, and `variation` describes which model the user reviewed. `verified_reviews` contains the text of each review, and `feedback` contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t).\n",
        "\n",
        "We are going develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the `feedback` column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfnR4ua2uLw"
      },
      "source": [
        "### 4.1 Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rS4hAiA05wQ"
      },
      "source": [
        "# Import additional packages\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxrlx4eD0Ygk",
        "outputId": "1a8935cd-3685-4651-bdd6-24278918dbc8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 359
        }
      },
      "source": [
        "# Load data\n",
        "url = \"https://raw.githubusercontent.com/michalis0/DataMining_and_MachineLearning/master/week6/data/amazon_alexa.tsv\"\n",
        "df = pd.read_csv(url, delimiter=\"\\t\")\n",
        "df.sample(10)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1401</th>\n",
              "      <td>5</td>\n",
              "      <td>31-Jul-18</td>\n",
              "      <td>Black  Show</td>\n",
              "      <td>Love the Echo Show.  Being able to see the lyr...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Plus</td>\n",
              "      <td>I love every aspect of the Echo Plus, I wanted...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2163</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Configuration: Fire TV Stick</td>\n",
              "      <td>Puts the pep back in my old TV.  All of the so...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2306</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Configuration: Fire TV Stick</td>\n",
              "      <td>I love my amazon sticks</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2097</th>\n",
              "      <td>5</td>\n",
              "      <td>29-Jun-18</td>\n",
              "      <td>Black  Plus</td>\n",
              "      <td>great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1569</th>\n",
              "      <td>4</td>\n",
              "      <td>29-Jul-18</td>\n",
              "      <td>Black  Show</td>\n",
              "      <td>I enjoy this thing. Its learning</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2737</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Dot</td>\n",
              "      <td>Dislike the volume. It does not sound loud eno...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2109</th>\n",
              "      <td>5</td>\n",
              "      <td>31-Jul-18</td>\n",
              "      <td>Configuration: Fire TV Stick</td>\n",
              "      <td>Using it to stream Amazon Prime and Netflix.</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2797</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>Only complaint I have is that the sound qualit...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>654</th>\n",
              "      <td>5</td>\n",
              "      <td>26-May-18</td>\n",
              "      <td>Black</td>\n",
              "      <td></td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      rating  ... feedback\n",
              "1401       5  ...        1\n",
              "1789       5  ...        1\n",
              "2163       5  ...        1\n",
              "2306       5  ...        1\n",
              "2097       5  ...        1\n",
              "1569       4  ...        1\n",
              "2737       5  ...        1\n",
              "2109       5  ...        1\n",
              "2797       5  ...        1\n",
              "654        5  ...        1\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNjV_B2E3nLd",
        "outputId": "2d14cf1b-0db1-4128-a9ae-f69124465159",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3150 entries, 0 to 3149\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   rating            3150 non-null   int64 \n",
            " 1   date              3150 non-null   object\n",
            " 2   variation         3150 non-null   object\n",
            " 3   verified_reviews  3150 non-null   object\n",
            " 4   feedback          3150 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 123.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6hCSlbq30UT"
      },
      "source": [
        "# Change date to datetime\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP8sYT2y34CU",
        "outputId": "7a1e9515-e179-4641-be9b-f9ab499d7e44",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3150 entries, 0 to 3149\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   rating            3150 non-null   int64         \n",
            " 1   date              3150 non-null   datetime64[ns]\n",
            " 2   variation         3150 non-null   object        \n",
            " 3   verified_reviews  3150 non-null   object        \n",
            " 4   feedback          3150 non-null   int64         \n",
            "dtypes: datetime64[ns](1), int64(2), object(2)\n",
            "memory usage: 123.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK8u3mNw35HC",
        "outputId": "7701ad2e-516a-4b1d-c705-aad86e8bc199",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Base rate: the data-set is unbalanced!\n",
        "df.feedback.value_counts()"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2893\n",
              "0     257\n",
              "Name: feedback, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d4NPeOq4A0X",
        "outputId": "82ecd096-cfd2-41b8-8d89-ebb526a41d68",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "round(df.feedback.value_counts()[1] / len(df), 4)"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iFd7_hl4eNJ"
      },
      "source": [
        "###### Tokening the Data With spaCy\n",
        "\n",
        "We create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stopwords.\n",
        "\n",
        "__A note from spacy documentation__: spaCy adds a special case for pronouns: all pronouns are lemmatized to the special token `-PRON-`. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrHhCTFN5ZXw",
        "outputId": "e5af8f39-9d39-4247-e23d-bf7bb1ebe2d4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Create a list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "punctuations"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCVM6xVQ5fP2",
        "outputId": "181fd40f-7a09-40af-f2fb-4ad5e000ded7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        }
      },
      "source": [
        "# Create a list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "list(stop_words)[:10]"
      ],
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['anywhere',\n",
              " 'above',\n",
              " 'both',\n",
              " 'last',\n",
              " 'otherwise',\n",
              " 'thence',\n",
              " 'does',\n",
              " 'seems',\n",
              " 'with',\n",
              " 'so']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ALYORw4LN0",
        "outputId": "bad8ad63-c3da-44e0-aa7a-b40b4aee3827",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Create tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Create token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatize each token and convert each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Remove stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # Return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n",
        "# Example\n",
        "review = df[\"verified_reviews\"].sample()\n",
        "review.values[0]"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'The product sounded the same as the emoji speaker from five below my sister has ... and even that one has Bluetooth and doesn’t need to be plugged in. The only good thing about this is that you can speak to it.'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOwdF_16JV-",
        "outputId": "041e7ade-7125-4559-d6ed-ea1bdc3f47a0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "spacy_tokenizer(review.values[0])"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['product',\n",
              " 'sound',\n",
              " 'emoji',\n",
              " 'speaker',\n",
              " 'sister',\n",
              " '...',\n",
              " 'bluetooth',\n",
              " 'need',\n",
              " 'plug',\n",
              " 'good',\n",
              " 'thing',\n",
              " 'speak']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5Yl8uR9eYv"
      },
      "source": [
        "#### Vectorization Feature Engineering (TF-IDF)\n",
        "\n",
        "We use the TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize the documents. This is a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-874sxt8iNM"
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer=spacy_tokenizer) # we use the above defined tokenizer"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO56WUV9yXn"
      },
      "source": [
        "### 4.2 Classification of the reviews using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7JkcGTb9ty9",
        "outputId": "344cc9da-2057-4c7c-ffff-6b44d7ad86b1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "# Select features\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "ylabels = df['feedback'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.2, random_state=1234)\n",
        "\n",
        "X_train"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2088    I really like this item. I already had the ech...\n",
              "287     Technology in such a small device. The price w...\n",
              "516           Pleased with the order just what I wanted!!\n",
              "832                                               Love it\n",
              "890                                   Works as advertised\n",
              "                              ...                        \n",
              "664     Great for mom its inside an owlhead in her jac...\n",
              "3125    This product is easy to use and very entertain...\n",
              "1318    You're going to have to shell out an extra $20...\n",
              "723     Fun item to play with and get used to using.  ...\n",
              "2863                                        Great product\n",
              "Name: verified_reviews, Length: 2520, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dbSYsF-HRi",
        "outputId": "bff1fe1d-c592-45cb-d136-081fbbf17b03",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 221
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2088    1\n",
              "287     1\n",
              "516     1\n",
              "832     1\n",
              "890     1\n",
              "       ..\n",
              "664     1\n",
              "3125    1\n",
              "1318    1\n",
              "723     1\n",
              "2863    1\n",
              "Name: feedback, Length: 2520, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8PkfsP-JYi",
        "outputId": "57d29ccb-ee57-43c3-f999-16647e887dad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...,\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7fedd1b6cd90>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07kHgHb-hC2"
      },
      "source": [
        "# Evaluate the model\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred)\n",
        "    recall = recall_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PjJWqHp_HvQ",
        "outputId": "7a92bc5e-531a-40a0-c782-94ec8182d11c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  2  49]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9222\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9220\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9594\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRrITD4y_nbb"
      },
      "source": [
        "For the test set, the model correctly identifies a sentiment 92.22% of the time. This is only slightly better than the base rate (91.84%). The difference lies in the two cases that the model correctly classifies as negative. Therefore, the model does not work very well. Maybe because it has too many positive reviews and cannot identify negative one among them.\n",
        "\n",
        "A recall of 1 means that if a sentiment is positive, it will be classififed as positive.\n",
        "\n",
        "We observe approximately the same on the training set, as shown below."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AeB_oSPKP7vZ",
        "outputId": "04001145-d7a1-4f6c-923c-da3095df42d9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 170
        }
      },
      "source": [
        "# Evaluate on training set\n",
        "evaluate(y_train, pipe.predict(X_train))"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[   6  200]\n",
            " [   0 2314]]\n",
            "ACCURACY SCORE:\n",
            "0.9206\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9204\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9586\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZAzALsgXu_v",
        "outputId": "cfd782dd-5039-489d-d47a-c076983f7f54",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "# Which reviews are classified as negative in test set?\n",
        "X_test.loc[pipe.predict(X_test) == 0]"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "491                            I've already returned it.\n",
              "381    It worked for a month or so then it stopped. I...\n",
              "Name: verified_reviews, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBMomG3RYeo2",
        "outputId": "f4c862b1-067d-4093-c30d-1a90bca424d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 136
        }
      },
      "source": [
        "# Which reviews are classified as negaive in training set?\n",
        "X_train.loc[pipe.predict(X_train) == 0]"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1348    Can't turn of &#34;things to try&#34; on the s...\n",
              "2611    This worked well for about 6 months but then s...\n",
              "551     product stopped working after return time ran out\n",
              "420     Item has never worked. Out of box it is broken...\n",
              "1903             Terrible. Stopped working after one day.\n",
              "2962    This worked well for about 6 months but then s...\n",
              "Name: verified_reviews, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9SnrtzOhS9cP",
        "outputId": "e04b7528-0cc7-4d76-e9cf-bba9883560dc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# Prediction for new reviews\n",
        "example_review_1 = \"I really love the product. It is very helpful. I use it everyday\" # positive\n",
        "example_review_2 = \"It stopped working, I want to return it\" # negative\n",
        "example_review_3 = \"I don't like it, it is bad\" # negative\n",
        "\n",
        "examples = pd.Series([example_review_1, example_review_2, example_review_3])\n",
        "examples"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    I really love the product. It is very helpful....\n",
              "1              It stopped working, I want to return it\n",
              "2                           I don't like it, it is bad\n",
              "dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yrnvd-ZPT7aR",
        "outputId": "11b04b2a-a630-4eaf-d2c9-e3aaf93187a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "pipe.predict(examples)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([1, 0, 1])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KHne-tgCScWk"
      },
      "source": [
        "### 4.3 BONUS\n",
        "In order to improve the prediction, we can:\n",
        "* Tun the hyperparameters\n",
        "* Use another classififer\n",
        "* Improve text preparation\n",
        "\n",
        "We illustrate the use of another classifier below. In addition, we try to predict the rating."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zypqialXQZTZ",
        "outputId": "c5bf3326-ec1f-4f53-f833-e8292b7c5902",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        }
      },
      "source": [
        "# BONUS 1: how to improve the classification: \n",
        "# --> tun your classififer or use another one\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define classifier\n",
        "classifier = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Fit model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "evaluate(y_test, y_pred)\n",
        "print(\"-----------------------\")\n",
        "\n",
        "# Evaluation - training set\n",
        "evaluate(y_train, pipe.predict(X_train))"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  9  42]\n",
            " [  0 579]]\n",
            "ACCURACY SCORE:\n",
            "0.9333\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9324\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9650\n",
            "-----------------------\n",
            "CONFUSION MATRIX:\n",
            "[[ 189   17]\n",
            " [   0 2314]]\n",
            "ACCURACY SCORE:\n",
            "0.9933\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9927\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9963\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ObKOjmwyZKvM",
        "outputId": "133d1191-40ee-4633-97af-5e1724ccb065",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Which reviews are classified as negative in test set?\n",
        "X_test.loc[pipe.predict(X_test) == 0].values"
      ],
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([\"I am not super impressed with Alexa. When my Prime lapsed, she wouldn't play anything. She isn't smart enough to differentiate among spotify accounts so we can't use it for that either. She randomly speaks up when nobody is talking to her. Just today I unplugged her...not sure I'll ever use my Alexa again.\",\n",
              "       \"It's got great sound and bass but it doesn't work all of the time. Its still hot or miss when it recognizes things\",\n",
              "       'The only negative we have on this product is the terrible sound quality.  A massive difference from the Alexa.  Which to us was a big reason we wanted to purchase this.Won’t be buying another until the speaker and sound quality can improve.',\n",
              "       \"I've already returned it.\",\n",
              "       'Returned from repair with No repair done. It has a problem which requires it to be on for a while for the defective part to show itself.I sent it for repair and got it back without being fixed.  The site gives no option to complain from the page.I have a echo spot which works perfectly in the same position.',\n",
              "       'Not much features.', 'The volume is very low',\n",
              "       'NOT CONNECTED TO MY PHONE PLAYLIST :(', 'Sad joke. Worthless.'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 48
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9D5xSc5_y9i",
        "outputId": "8f6ffeef-c6d6-49de-cbc1-76b00cbf20e0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        }
      },
      "source": [
        "# BONUS 2: predict the rating\n",
        "df.sample(5)"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>44</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-30</td>\n",
              "      <td>Sandstone Fabric</td>\n",
              "      <td>Great</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1180</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-28</td>\n",
              "      <td>White  Spot</td>\n",
              "      <td>Great!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2925</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-30</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>Love Alexa. Love the Echo Dot but we have trou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2784</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-30</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>I loved it does exactly what it says</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1495</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-30</td>\n",
              "      <td>Black  Show</td>\n",
              "      <td>Love it!  Great to request music and be able t...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      rating  ... feedback\n",
              "44         5  ...        1\n",
              "1180       5  ...        1\n",
              "2925       4  ...        1\n",
              "2784       5  ...        1\n",
              "1495       5  ...        1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiyyP7T6Bykf",
        "outputId": "0c08cc0c-8cce-4856-b70c-5034a1d1c9c8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 119
        }
      },
      "source": [
        "df.rating.value_counts()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    2286\n",
              "4     455\n",
              "1     161\n",
              "3     152\n",
              "2      96\n",
              "Name: rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNNejlJgB2aY",
        "outputId": "955c6c41-f3dd-4ef9-9cba-1ffcf9865e42",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Base rate\n",
        "round(df.rating.value_counts()[5] / len(df), 4)"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZE1JqorB9Lb",
        "outputId": "4675a45d-35e4-4e1a-d361-7349d4df6a7a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# Select features\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "y = df['rating'] # the labels, or answers, we want to test against\n",
        "\n",
        "# Train test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1234)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  2   0   0   4  31]\n",
            " [  0   1   0   2  11]\n",
            " [  0   0   0   3  32]\n",
            " [  0   0   0   5  87]\n",
            " [  0   0   0   5 447]]\n",
            "ACCURACY SCORE:\n",
            "0.7222\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfE1iXi0Eeuz",
        "outputId": "15807468-c4a5-4a52-8340-db1f645f3d73",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 153
        }
      },
      "source": [
        "# BONUS 3: use random forest\n",
        "\n",
        "# Define classifier\n",
        "classifier = RandomForestClassifier(n_estimators=50)\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation - test set\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  7   1   1   2  26]\n",
            " [  0   4   0   0  10]\n",
            " [  0   0  12   4  19]\n",
            " [  0   0   0  34  58]\n",
            " [  0   0   1   3 448]]\n",
            "ACCURACY SCORE:\n",
            "0.8016\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFt-MD98E1E4"
      },
      "source": [
        ""
      ],
      "execution_count": 53,
      "outputs": []
    }
  ]
}