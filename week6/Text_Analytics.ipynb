{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Text_Analytics.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "h0VsVUPLT4Tn",
        "RXlVy05IUC-D",
        "te403rkXbGKf",
        "Q_Z6MiCtTp8n",
        "imbw_TUkTp8v",
        "c7J5Na4DTp84",
        "1QZOutDwsYij",
        "wCFQrtHLtMTt",
        "wzfnR4ua2uLw"
      ],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/DataMining_and_MachineLearning/blob/master/week6/Text_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "onO46LysT2dh"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 6\n",
        "# Text Analytics\n",
        "\n",
        "[Text Analytics](https://people.ischool.berkeley.edu/~hearst/text-mining.html) (or text mining) is the process of deriving high-quality information from text. It involves \"the discovery by computer of new, previously unknown information, by automatically extracting information from different written resources.\" Written resources may include websites, books, emails, reviews, and articles.\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. Summary\n",
        "* 1.1 Applications\n",
        "* 1.2 Tokenization and Stopwords\n",
        "* 1.3 Stemming and Lemmatization\n",
        "* 1.4 Text Representation\n",
        "\n",
        "#### 2. Text Preparation\n",
        "* 2.1 Install spaCy\n",
        "* 2.2 Tokenization\n",
        "* 2.3 Dependency Parsing\n",
        "* 2.4 Remove Stopwords\n",
        "* 2.5 Lemmatization\n",
        "* 2.6 Entity Detection \n",
        "\n",
        "#### 3. Text Representation\n",
        "* 3.1 Bag of Words (BOW)\n",
        "* 3.2 TF-IDF Representation\n",
        "\n",
        "#### 4. Text Classification: Alexa Reviews\n",
        "* 4.1 Load and prepare data\n",
        "* 4.2 Classification of the reviews using logistic regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h0VsVUPLT4Tn"
      },
      "source": [
        "## 1. Summary"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cHvrJj5yT8uQ"
      },
      "source": [
        "### 1.1 Applications\n",
        "There are many applications of text analytics, for example:\n",
        "* Search for relevant websites or articles using a search engine\n",
        "* Sentiment Analysis (e.g. classify tweets or film reviews as positive, neutral or negative)\n",
        "* Chatbots (e.g. Siri, Alexa)\n",
        "* Project idea: The Impact of Donald Trump’s Tweets on Financial.\n",
        "* Etc.\n",
        "\n",
        "### 1.2 Tokenization and Stopwords\n",
        "Tokens are the elementary building blocks (words, numbers, characters) in a document. Tokenization is the process of splitting an input\n",
        "sequence into tokens. Example: \"I love data science\" --> \"I\", \"love\", \"data\", \"science\". Stopwords are common words that appear very frequently (e.g. \"is\", \"and\", \"you\", etc.). It is convenient to remove them as they do not add much to the content of a document and are therefore generallny not useful for text analysis.\n",
        "\n",
        "### 1.3 Lemmatization and Stemming\n",
        "* Goal: have the same token for different forms of a word (e.g. fishing, fished, fisher, fishers, etc.)\n",
        "* Lemmatization: Find what is the lemma of a word (e.g. feet -> foot)\n",
        "* Stemming: one method for lemmatization where rules that remove the ending of a word are applied (e.g. fish)\n",
        "\n",
        "\n",
        "### 1.4 Text Representation\n",
        "* Goal: transform text such that it can be used for text analysis\n",
        "* Bag of Words (BOW): works in many case but order is not preserved (solution: n-grams).\n",
        "* TF-IDF: emphasizes important words."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RXlVy05IUC-D"
      },
      "source": [
        "## 2. Text Preparation\n",
        "In this section, we explain how to prepare a text for analysis. This includes tockeninzing the text, removing stopwords, etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EA4tt9roTp8T"
      },
      "source": [
        "### 2.1 Install spaCy\n",
        "[spaCy](https://spacy.io/) is an open-source natural language processing library for Python. It is designed particularly for production use, and it can help us to build applications that process massive volumes of text efficiently.\n",
        "\n",
        "We install the library and its English-language model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gf6I0JIYTp8U",
        "outputId": "f5d5319c-092d-4d1c-8db4-d0bdd95d7650",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Install and update spaCy\n",
        "!pip install -U spacy\n",
        "\n",
        "# Load the english language model\n",
        "!python -m spacy download en"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already up-to-date: spacy in /usr/local/lib/python3.6/dist-packages (2.3.2)\n",
            "Requirement already satisfied, skipping upgrade: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.23.0)\n",
            "Requirement already satisfied, skipping upgrade: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (3.0.2)\n",
            "Requirement already satisfied, skipping upgrade: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied, skipping upgrade: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.0)\n",
            "Requirement already satisfied, skipping upgrade: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy) (50.3.0)\n",
            "Requirement already satisfied, skipping upgrade: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (0.8.0)\n",
            "Requirement already satisfied, skipping upgrade: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy) (7.4.1)\n",
            "Requirement already satisfied, skipping upgrade: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.0.2)\n",
            "Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (4.41.1)\n",
            "Requirement already satisfied, skipping upgrade: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.0.3)\n",
            "Requirement already satisfied, skipping upgrade: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.18.5)\n",
            "Requirement already satisfied, skipping upgrade: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.10)\n",
            "Requirement already satisfied, skipping upgrade: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.24.3)\n",
            "Requirement already satisfied, skipping upgrade: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n",
            "Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2020.6.20)\n",
            "Requirement already satisfied, skipping upgrade: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy) (2.0.0)\n",
            "Requirement already satisfied, skipping upgrade: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy) (3.2.0)\n",
            "Requirement already satisfied: en_core_web_sm==2.3.1 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.3.1/en_core_web_sm-2.3.1.tar.gz#egg=en_core_web_sm==2.3.1 in /usr/local/lib/python3.6/dist-packages (2.3.1)\n",
            "Requirement already satisfied: spacy<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from en_core_web_sm==2.3.1) (2.3.2)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.3)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.4.1)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.1.3)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (4.41.1)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.18.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (50.3.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (0.8.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.23.0)\n",
            "Requirement already satisfied: thinc==7.4.1 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (7.4.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.0.2)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.2)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.0.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (2020.6.20)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (1.24.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy<2.4.0,>=2.3.0->en_core_web_sm==2.3.1) (3.2.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;2m✔ Linking successful\u001b[0m\n",
            "/usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n",
            "/usr/local/lib/python3.6/dist-packages/spacy/data/en\n",
            "You can now load the model via spacy.load('en')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FhOeeRlqTp8Z"
      },
      "source": [
        "# Import required packages\n",
        "import spacy\n",
        "from spacy import displacy\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "import pandas as pd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XUibE-SyTp8d"
      },
      "source": [
        "### 2.2 Tokenization\n",
        "\n",
        "Tokenization is the process of breaking a text into pieces called tokens. A token simply refers to an individual part of a sentence having some semantic value. SpaCy‘s tokenizer takes input in form of unicode text and outputs a sequence of token objects. In addition, SpaCy automatically breaks your document into tokens when a document is created using the language model.\n",
        "\n",
        "Let’s take a look at a simple example. Imagine we have the following text, and we’d like to tokenize it:\n",
        "\n",
        "> When learning data science, you shouldn't get discouraged!\n",
        "\n",
        "> Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\n",
        "\n",
        "There are a couple of different ways we can appoach this. The first is called __word tokenization__, which means breaking up the text into individual words. This is a critical step for many language processing applications, as they often require inputs in the form of individual words rather than longer strings of text."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dj9OfB1BTp8e",
        "outputId": "56dd8209-eaf0-4fc9-eb88-271cb93d9ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Declare the text\n",
        "text = \"\"\"When learning data science, you shouldn't get discouraged!\n",
        "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!\"\"\"\n",
        "\n",
        "# spaCy object is used to create documents with linguistic annotations.\n",
        "my_doc = sp(text)\n",
        "\n",
        "my_doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "When learning data science, you shouldn't get discouraged!\n",
              "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hc7fwTbPZdXJ",
        "outputId": "602c0792-cff0-46ad-be1c-551f98ca748e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "type(my_doc)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "spacy.tokens.doc.Doc"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ex7-T-BkYXZM",
        "outputId": "26acf0fd-3d20-4189-89b0-42500f05ed5e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create list of word tokens\n",
        "token_list = []\n",
        "\n",
        "for token in my_doc:\n",
        "    token_list.append(token.text)\n",
        "\n",
        "token_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['When',\n",
              " 'learning',\n",
              " 'data',\n",
              " 'science',\n",
              " ',',\n",
              " 'you',\n",
              " 'should',\n",
              " \"n't\",\n",
              " 'get',\n",
              " 'discouraged',\n",
              " '!',\n",
              " '\\n',\n",
              " 'Challenges',\n",
              " 'and',\n",
              " 'setbacks',\n",
              " 'are',\n",
              " \"n't\",\n",
              " 'failures',\n",
              " ',',\n",
              " 'they',\n",
              " \"'re\",\n",
              " 'just',\n",
              " 'part',\n",
              " 'of',\n",
              " 'the',\n",
              " 'journey',\n",
              " '.',\n",
              " 'You',\n",
              " \"'ve\",\n",
              " 'got',\n",
              " 'this',\n",
              " '!']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xi29AcSsTp8j"
      },
      "source": [
        "\n",
        "As we can see, spaCy produces a list that contains each token as a separate item. Notice that it has recognized that contractions such as _shouldn’t_ actually represent two distinct words, and it has thus broken them down into two distinct tokens.\n",
        "\n",
        "In the example above, we first load language dictionaries. Here we load the english dictionary using the English() class and create an object of this class, “nlp”, which is used to create documents with linguistic annotations and various language properties. After creating the document, we create a list of tokens.\n",
        "\n",
        "We can also see the parts-of-speech (POS) of each of these tokens using the `.pos_` attribute shown below. POS tagging can be really useful, particularly if you have words or tokens that can have multiple POS tags. For instance, the word \"fish\" can be used as both a noun and verb, depending upon the context."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NvkGwDJoWoIs",
        "outputId": "4caf6f2f-6df9-448b-d365-c83a3d71c83e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# POS\n",
        "for word in my_doc:\n",
        "    print(word.text, word.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "When ADV\n",
            "learning VERB\n",
            "data NOUN\n",
            "science NOUN\n",
            ", PUNCT\n",
            "you PRON\n",
            "should VERB\n",
            "n't PART\n",
            "get AUX\n",
            "discouraged ADJ\n",
            "! PUNCT\n",
            "\n",
            " SPACE\n",
            "Challenges NOUN\n",
            "and CCONJ\n",
            "setbacks NOUN\n",
            "are AUX\n",
            "n't PART\n",
            "failures NOUN\n",
            ", PUNCT\n",
            "they PRON\n",
            "'re AUX\n",
            "just ADV\n",
            "part NOUN\n",
            "of ADP\n",
            "the DET\n",
            "journey NOUN\n",
            ". PUNCT\n",
            "You PRON\n",
            "'ve AUX\n",
            "got VERB\n",
            "this DET\n",
            "! PUNCT\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cUsKHgp7Y3yM",
        "outputId": "59ebe405-5de3-4d47-e551-8e0c033ae514",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Another example\n",
        "doc1 = sp(\"I like to fish\") # verb\n",
        "doc2 = sp(\"I eat a fish\") # noun\n",
        "\n",
        "for word in doc1:\n",
        "  print(word.text, word.pos_)\n",
        "\n",
        "print(\"-----------------\")\n",
        "\n",
        "for word in doc2:\n",
        "  print(word.text, word.pos_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "I PRON\n",
            "like VERB\n",
            "to PART\n",
            "fish VERB\n",
            "-----------------\n",
            "I PRON\n",
            "eat VERB\n",
            "a DET\n",
            "fish NOUN\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dCiU1-PDXKpp"
      },
      "source": [
        "\n",
        "If we want, we can also break the text into sentences rather than words. This is called __sentence tokenization__. When performing sentence tokenization, the tokenizer looks for specific characters that fall between sentences, like periods, exclaimation points, and newline characters. For sentence tokenization, we will use a preprocessing pipeline because sentence preprocessing using spaCy includes a tokenizer, a tagger, a parser and an entity recognizer that we need to access to correctly identify what’s a sentence and what isn’t.\n",
        "\n",
        "In the code below, spaCy tokenizes the text and creates a Doc object. This Doc object uses our preprocessing pipeline’s components tagger, parser and entity recognizer to break the text down into components. From this pipeline we can extract any component, but here we’re going to access sentence tokens using the sentencizer component."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "odi_WkNZTp8k",
        "outputId": "2f197438-ed69-4650-98db-a1945560a69e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# create list of sentence tokens\n",
        "sents_list = []\n",
        "\n",
        "for sent in my_doc.sents:\n",
        "    sents_list.append(sent.text)\n",
        "\n",
        "sents_list"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[\"When learning data science, you shouldn't get discouraged!\\n\",\n",
              " \"Challenges and setbacks aren't failures, they're just part of the journey.\",\n",
              " \"You've got this!\"]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "te403rkXbGKf"
      },
      "source": [
        "### 2.3 Dependency Parsing\n",
        "__Depenency parsing__ is a language processing technique that allows to better determine the meaning of a sentence by analyzing how it’s constructed to determine how the individual words relate to each other.\n",
        "\n",
        "Consider, for example, the sentence “Joe throws the ball.” We have two nouns (Joe and ball) and one verb (throws). But we can’t just look at these words individually, or we may end up thinking that the ball is throwing Bill! To understand the sentence correctly, we need to look at the word order and sentence structure, not just the words and their parts of speech.\n",
        "\n",
        "Below, we have a short sentence. We’ll use a spaCy method called `noun_chunks`, which breaks the input down into nouns and the words describing them, and iterate through each chunk in our source text, identifying the word, its root, its dependency identification, and which chunk it belongs to."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YFLButK_bOor",
        "outputId": "44cf36e0-a9e8-4b47-bf89-7c32e964d264",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "doc = sp(\" Joe threw a ball and President Donald, in pursuit of the ball, hit a wall.\") # notice the space at the beginning\n",
        "\n",
        "for chunk in doc.noun_chunks:\n",
        "    print(chunk.text, chunk.root.text, chunk.root.dep_,\n",
        "          chunk.root.head.text)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            " Joe Joe nsubj threw\n",
            "a ball ball dobj threw\n",
            "President Donald Donald conj ball\n",
            "pursuit pursuit pobj in\n",
            "the ball ball pobj of\n",
            "a wall wall dobj hit\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7VmqTQQ9bYTl",
        "outputId": "485d2dd7-4bfd-41a6-d11c-5bbcc426fbf8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Let's visualize this\n",
        "displacy.render(doc, style=\"dep\", jupyter= True, options={'distance': 120})"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><svg xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\" xml:lang=\"en\" id=\"2c97546322c14b6dadb8b4b1dbdea03a-0\" class=\"displacy\" width=\"1970\" height=\"437.0\" direction=\"ltr\" style=\"max-width: none; height: 437.0px; color: #000000; background: #ffffff; font-family: Arial; direction: ltr\">\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"50\"> </tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"50\">SPACE</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"170\">Joe</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"170\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"290\">threw</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"290\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"410\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"410\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"530\">ball</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"530\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"650\">and</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"650\">CCONJ</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"770\">President</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"770\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"890\">Donald,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"890\">PROPN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1010\">in</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1010\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1130\">pursuit</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1130\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1250\">of</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1250\">ADP</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1370\">the</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1370\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1490\">ball,</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1490\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1610\">hit</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1610\">VERB</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1730\">a</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1730\">DET</tspan>\n",
              "</text>\n",
              "\n",
              "<text class=\"displacy-token\" fill=\"currentColor\" text-anchor=\"middle\" y=\"347.0\">\n",
              "    <tspan class=\"displacy-word\" fill=\"currentColor\" x=\"1850\">wall.</tspan>\n",
              "    <tspan class=\"displacy-tag\" dy=\"2em\" fill=\"currentColor\" x=\"1850\">NOUN</tspan>\n",
              "</text>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-0\" stroke-width=\"2px\" d=\"M70,302.0 C70,242.0 150.0,242.0 150.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-0\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\"></textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M70,304.0 L62,292.0 78,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-1\" stroke-width=\"2px\" d=\"M190,302.0 C190,242.0 270.0,242.0 270.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-1\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">nsubj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M190,304.0 L182,292.0 198,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-2\" stroke-width=\"2px\" d=\"M430,302.0 C430,242.0 510.0,242.0 510.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-2\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M430,304.0 L422,292.0 438,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-3\" stroke-width=\"2px\" d=\"M310,302.0 C310,182.0 515.0,182.0 515.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-3\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M515.0,304.0 L523.0,292.0 507.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-4\" stroke-width=\"2px\" d=\"M550,302.0 C550,242.0 630.0,242.0 630.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-4\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">cc</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M630.0,304.0 L638.0,292.0 622.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-5\" stroke-width=\"2px\" d=\"M790,302.0 C790,242.0 870.0,242.0 870.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-5\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">compound</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M790,304.0 L782,292.0 798,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-6\" stroke-width=\"2px\" d=\"M550,302.0 C550,122.0 880.0,122.0 880.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-6\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M880.0,304.0 L888.0,292.0 872.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-7\" stroke-width=\"2px\" d=\"M310,302.0 C310,62.0 1005.0,62.0 1005.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-7\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1005.0,304.0 L1013.0,292.0 997.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-8\" stroke-width=\"2px\" d=\"M1030,302.0 C1030,242.0 1110.0,242.0 1110.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-8\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1110.0,304.0 L1118.0,292.0 1102.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-9\" stroke-width=\"2px\" d=\"M1150,302.0 C1150,242.0 1230.0,242.0 1230.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-9\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">prep</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1230.0,304.0 L1238.0,292.0 1222.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-10\" stroke-width=\"2px\" d=\"M1390,302.0 C1390,242.0 1470.0,242.0 1470.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-10\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1390,304.0 L1382,292.0 1398,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-11\" stroke-width=\"2px\" d=\"M1270,302.0 C1270,182.0 1475.0,182.0 1475.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-11\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">pobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1475.0,304.0 L1483.0,292.0 1467.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-12\" stroke-width=\"2px\" d=\"M310,302.0 C310,2.0 1610.0,2.0 1610.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-12\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">conj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1610.0,304.0 L1618.0,292.0 1602.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-13\" stroke-width=\"2px\" d=\"M1750,302.0 C1750,242.0 1830.0,242.0 1830.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-13\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">det</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1750,304.0 L1742,292.0 1758,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "\n",
              "<g class=\"displacy-arrow\">\n",
              "    <path class=\"displacy-arc\" id=\"arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-14\" stroke-width=\"2px\" d=\"M1630,302.0 C1630,182.0 1835.0,182.0 1835.0,302.0\" fill=\"none\" stroke=\"currentColor\"/>\n",
              "    <text dy=\"1.25em\" style=\"font-size: 0.8em; letter-spacing: 1px\">\n",
              "        <textPath xlink:href=\"#arrow-2c97546322c14b6dadb8b4b1dbdea03a-0-14\" class=\"displacy-label\" startOffset=\"50%\" side=\"left\" fill=\"currentColor\" text-anchor=\"middle\">dobj</textPath>\n",
              "    </text>\n",
              "    <path class=\"displacy-arrowhead\" d=\"M1835.0,304.0 L1843.0,292.0 1827.0,292.0\" fill=\"currentColor\"/>\n",
              "</g>\n",
              "</svg></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_Z6MiCtTp8n"
      },
      "source": [
        "### 2.4 Remove Stopwords\n",
        "Most text data that we work with is going to contain a lot of words that aren’t actually useful to us. These words, called stopwords, are useful in human speech, but they don’t have much to contribute to data analysis. Removing stopwords helps us eliminate noise and distraction from our text data, and also speeds up the time analysis takes (since there are fewer words to process).\n",
        "\n",
        "Let’s take a look at the stopwords spaCy includes by default."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_kB8kI0eTp8o",
        "outputId": "c40b18ac-e693-49b6-c08d-dac4481aea7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Import stopwords from English language\n",
        "spacy_stopwords = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "# Print total number of stopwords\n",
        "print('Number of stop words: %d' % len(spacy_stopwords))\n",
        "\n",
        "# Print first 20 stopwords\n",
        "print('First 20 stop words: %s' % list(spacy_stopwords)[:20])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of stop words: 326\n",
            "First 20 stop words: ['so', 'cannot', 'everyone', 'through', 'eight', 'two', 'whereafter', 'an', 'regarding', 'anyone', 'onto', 'someone', 'three', 'during', 'five', 'always', 'they', 'does', 'neither', 'only']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aw0Wfvu4Tp8q"
      },
      "source": [
        "Now that we’ve got our list of stopwords, let’s use it to remove the stopwords from the text string we were working on in the previous section."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eq5yvoZtlhsd",
        "outputId": "f82755b5-8bf7-475f-9826-4aa10c998388",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "my_doc"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "When learning data science, you shouldn't get discouraged!\n",
              "Challenges and setbacks aren't failures, they're just part of the journey. You've got this!"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y54Kiz9zTp8r",
        "outputId": "28355b2f-c8eb-439b-80b3-5aa5a0a61444",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Declare list for filtered sentence\n",
        "filtered_sent = []\n",
        "\n",
        "# Filter stopwords\n",
        "for word in my_doc:\n",
        "    if word.is_stop == False:\n",
        "        filtered_sent.append(word)\n",
        "\n",
        "filtered_sent"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[learning,\n",
              " data,\n",
              " science,\n",
              " ,,\n",
              " discouraged,\n",
              " !,\n",
              " ,\n",
              " Challenges,\n",
              " setbacks,\n",
              " failures,\n",
              " ,,\n",
              " journey,\n",
              " .,\n",
              " got,\n",
              " !]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UOvM2NFdl29W",
        "outputId": "574760e4-33e2-4bab-9b30-779980e2314c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# We can also remove the punctuation\n",
        "filtered_sent2 = []\n",
        "removed_tokens = []\n",
        "\n",
        "# Filter stopwords and punctuation\n",
        "for word in my_doc:\n",
        "  if (word.is_stop == True) or (word.is_punct == True):\n",
        "    removed_tokens.append(word)\n",
        "  else:\n",
        "    filtered_sent2.append(word)\n",
        "\n",
        "removed_tokens"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[When,\n",
              " ,,\n",
              " you,\n",
              " should,\n",
              " n't,\n",
              " get,\n",
              " !,\n",
              " and,\n",
              " are,\n",
              " n't,\n",
              " ,,\n",
              " they,\n",
              " 're,\n",
              " just,\n",
              " part,\n",
              " of,\n",
              " the,\n",
              " .,\n",
              " You,\n",
              " 've,\n",
              " this,\n",
              " !]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mw-ebvZom1EP",
        "outputId": "d1cbb83c-dc7e-4e49-bd01-f49a953506da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "filtered_sent2"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[learning,\n",
              " data,\n",
              " science,\n",
              " discouraged,\n",
              " ,\n",
              " Challenges,\n",
              " setbacks,\n",
              " failures,\n",
              " journey,\n",
              " got]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "imbw_TUkTp8v"
      },
      "source": [
        "### 2.5 Lemmatization\n",
        "Lemmatization is a way of dealing with the fact that while words like connect, connection, connecting, connected, etc. aren’t exactly the same, they all have the same essential meaning: connect. The differences in spelling have grammatical functions in spoken language, but for machine processing, those differences can be confusing, so we need a way to change all the words that are forms of the word connect into the word connect itself.\n",
        "\n",
        "One method for doing this is called __stemming__. Stemming involves simply lopping off easily-identified prefixes and suffixes to produce what’s often the simplest version of a word, the root. Connection, for example, would have the -ion suffix removed and be correctly reduced to connect. This kind of simple stemming is often all that’s needed, but lemmatization—which actually looks at words and their roots (called lemma) as described in the dictionary—is more precise (as long as the words exist in the dictionary).\n",
        "\n",
        "Let's look at this simple example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UTqSImYfTp8v",
        "outputId": "085469bc-9cbf-4f1c-dc7d-ffb94dc4cb23",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Lemmatization\n",
        "lem = sp(\"run runs ran running runner runners\")\n",
        "\n",
        "# Find lemma for each word\n",
        "for word in lem:\n",
        "    print(word.text, word.lemma_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "run run\n",
            "runs run\n",
            "ran run\n",
            "running run\n",
            "runner runner\n",
            "runners runner\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c7J5Na4DTp84"
      },
      "source": [
        "### 2.6 Entity Detection\n",
        "\n",
        "__Entity detection__, also called entity recognition, is a more advanced form of language processing that identifies important elements like places, people, organizations, and languages within a text. This is really helpful for quickly extracting information from the text, since you can quickly pick out important topics or indentify key sections of it.\n",
        "\n",
        "Let’s try out some entity detection using a few paragraphs from this [article](https://www.bloomberg.com/features/trump-tweets-market/)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nvlb9S71Tp84",
        "outputId": "568667b3-6e80-4b70-d58b-e6f1bc47f416",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "article = sp(\"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.\n",
        "\n",
        "Before U.S. stocks peaked in late January, he drew a direct connection between the increase in market value of American companies and his administration’s pro-growth policies on more than 10 occasions in that month alone.\n",
        "\"\"\")\n",
        "\n",
        "entities = [(i, i.label_, i.label) for i in article.ents]\n",
        "entities"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[(Donald Trump, 'PERSON', 380),\n",
              " (Twitter, 'ORG', 383),\n",
              " (American, 'NORP', 381),\n",
              " (U.S., 'GPE', 384),\n",
              " (late January, 'DATE', 391),\n",
              " (American, 'NORP', 381),\n",
              " (more than 10, 'CARDINAL', 397),\n",
              " (that month alone, 'DATE', 391)]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W1ScYkq7Tp87"
      },
      "source": [
        "example above shows spacy is able to identify a variety of different entity types, including specific locations (GPE), date-related words (DATE), important numbers (CARDINAL), specific individuals (PERSON), etc.\n",
        "\n",
        "Using `displaCy` we can also visualize the text, with each identified entity highlighted by color and labeled. We’ll use `style = \"ent\"` to tell displaCy that we want to visualize entities here."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hGDOBT6zTp88",
        "outputId": "5aa229e9-d045-4a44-8816-43f92da81752",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "displacy.render(article, style = \"ent\", jupyter = True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\"></br>President \n",
              "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Donald Trump\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
              "</mark>\n",
              " gets a lot of attention for using \n",
              "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    Twitter\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
              "</mark>\n",
              " to attack \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.</br></br>Before \n",
              "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    U.S.\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
              "</mark>\n",
              " stocks peaked in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    late January\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ", he drew a direct connection between the increase in market value of \n",
              "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    American\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
              "</mark>\n",
              " companies and his administration’s pro-growth policies on \n",
              "<mark class=\"entity\" style=\"background: #e4e7d2; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    more than 10\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">CARDINAL</span>\n",
              "</mark>\n",
              " occasions in \n",
              "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    that month alone\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
              "</mark>\n",
              ".\n",
              "</div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1QZOutDwsYij"
      },
      "source": [
        "## 3. Text Representation\n",
        "We now show how to transform a text into an usable into for text classification. We use the article from the last section and two other sentences."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X2MjeqoQxdOx"
      },
      "source": [
        "# Article as a string, not a spacy object\n",
        "article = \"\"\"\n",
        "President Donald Trump gets a lot of attention for using Twitter to attack American trading partners, political foes, and media companies. But he often takes to the platform to celebrate the strength of the world’s largest economy and its publicly-traded companies.\n",
        "\n",
        "Before U.S. stocks peaked in late January, he drew a direct connection between the increase in market value of American companies and his administration’s pro-growth policies on more than 10 occasions in that month alone.\n",
        "\"\"\"\n",
        "\n",
        "# Sentences\n",
        "s1 = \"\"\"Donald Trump is a great friend, and he has four or five Picassos on his plane. And that's where I would look at them.\"\"\" # from Shaquille O'Neal\n",
        "s2 = \"\"\"Donald Trump is a phony, a fraud. His promises are as worthless as a degree from Trump University.\"\"\" # from Mitt Romney\n",
        "\n",
        "texts = [article, s1, s2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCFQrtHLtMTt"
      },
      "source": [
        "### 3.1 Bag of Words (BOW)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZyzBm0BUtLc0",
        "outputId": "d43e4c83-4b13-48c4-9319-f8a50a792a0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using default tokenizer \n",
        "count = CountVectorizer(ngram_range=(1,2), stop_words=\"english\")\n",
        "bow = count.fit_transform(texts)\n",
        "\n",
        "# Show feature matrix\n",
        "bow.toarray()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 3, 1, 1, 1, 1, 1, 0, 0, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0,\n",
              "        0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 1, 1, 1, 1, 1, 1, 1,\n",
              "        1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1,\n",
              "        1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
              "       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "        0, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0,\n",
              "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0,\n",
              "        0, 0, 0, 0, 0, 2, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GRmUUc0kzi3_",
        "outputId": "f96d6b80-db2d-4707-8331-53520083535f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Get feature names\n",
        "feature_names = count.get_feature_names()\n",
        "\n",
        "# View feature names\n",
        "feature_names[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['10',\n",
              " '10 occasions',\n",
              " 'administration',\n",
              " 'administration pro',\n",
              " 'american',\n",
              " 'american companies',\n",
              " 'american trading',\n",
              " 'attack',\n",
              " 'attack american',\n",
              " 'attention']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sxCSvGUTz2tl",
        "outputId": "e578ea72-3773-41a3-a228-39321cf6e597",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Show as a dataframe\n",
        "pd.DataFrame(\n",
        "    bow.todense(), \n",
        "    columns=feature_names\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>10 occasions</th>\n",
              "      <th>administration</th>\n",
              "      <th>administration pro</th>\n",
              "      <th>american</th>\n",
              "      <th>american companies</th>\n",
              "      <th>american trading</th>\n",
              "      <th>attack</th>\n",
              "      <th>attack american</th>\n",
              "      <th>attention</th>\n",
              "      <th>attention using</th>\n",
              "      <th>celebrate</th>\n",
              "      <th>celebrate strength</th>\n",
              "      <th>companies</th>\n",
              "      <th>companies administration</th>\n",
              "      <th>companies stocks</th>\n",
              "      <th>companies takes</th>\n",
              "      <th>connection</th>\n",
              "      <th>connection increase</th>\n",
              "      <th>degree</th>\n",
              "      <th>degree trump</th>\n",
              "      <th>direct</th>\n",
              "      <th>direct connection</th>\n",
              "      <th>donald</th>\n",
              "      <th>donald trump</th>\n",
              "      <th>drew</th>\n",
              "      <th>drew direct</th>\n",
              "      <th>economy</th>\n",
              "      <th>economy publicly</th>\n",
              "      <th>foes</th>\n",
              "      <th>foes media</th>\n",
              "      <th>fraud</th>\n",
              "      <th>fraud promises</th>\n",
              "      <th>friend</th>\n",
              "      <th>friend picassos</th>\n",
              "      <th>gets</th>\n",
              "      <th>gets lot</th>\n",
              "      <th>great</th>\n",
              "      <th>great friend</th>\n",
              "      <th>growth</th>\n",
              "      <th>...</th>\n",
              "      <th>platform</th>\n",
              "      <th>platform celebrate</th>\n",
              "      <th>policies</th>\n",
              "      <th>policies 10</th>\n",
              "      <th>political</th>\n",
              "      <th>political foes</th>\n",
              "      <th>president</th>\n",
              "      <th>president donald</th>\n",
              "      <th>pro</th>\n",
              "      <th>pro growth</th>\n",
              "      <th>promises</th>\n",
              "      <th>promises worthless</th>\n",
              "      <th>publicly</th>\n",
              "      <th>publicly traded</th>\n",
              "      <th>stocks</th>\n",
              "      <th>stocks peaked</th>\n",
              "      <th>strength</th>\n",
              "      <th>strength world</th>\n",
              "      <th>takes</th>\n",
              "      <th>takes platform</th>\n",
              "      <th>traded</th>\n",
              "      <th>traded companies</th>\n",
              "      <th>trading</th>\n",
              "      <th>trading partners</th>\n",
              "      <th>trump</th>\n",
              "      <th>trump gets</th>\n",
              "      <th>trump great</th>\n",
              "      <th>trump phony</th>\n",
              "      <th>trump university</th>\n",
              "      <th>twitter</th>\n",
              "      <th>twitter attack</th>\n",
              "      <th>university</th>\n",
              "      <th>using</th>\n",
              "      <th>using twitter</th>\n",
              "      <th>value</th>\n",
              "      <th>value american</th>\n",
              "      <th>world</th>\n",
              "      <th>world largest</th>\n",
              "      <th>worthless</th>\n",
              "      <th>worthless degree</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>2</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 109 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "   10  10 occasions  administration  ...  world largest  worthless  worthless degree\n",
              "0   1             1               1  ...              1          0                 0\n",
              "1   0             0               0  ...              0          0                 0\n",
              "2   0             0               0  ...              0          1                 1\n",
              "\n",
              "[3 rows x 109 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v7tNGrTutQ2r"
      },
      "source": [
        "### 3.2 TF-IDF Representation\n",
        "\n",
        "\n",
        "Recall that:\n",
        "\n",
        "- term frequency tf = count(word, document) / len(document) \n",
        "- term frequency idf = log( len(collection) / count(document_containing_term, collection) )\n",
        "- tf-idf = tf * idf \n",
        "\n",
        "It is important to mention that the IDF value for a word remains the same throughout all the documents as it depends upon the total number of documents. On the other hand, TF values of a word differ from document to document.\n",
        "\n",
        "The TF for the word \"car\" is 1/7.\n",
        "\n",
        "Let's find the IDF frequency of the word \"car\". Since we have 2 documents and the word \"car\" occurs in 1 of them, therefore the IDF value of the word \"car\" is log(2/1) = 1.66.\n",
        "\n",
        "\n",
        "\n",
        "Finally, the TF-IDF values are calculated by multiplying TF values with their corresponding IDF values.\n",
        "\n",
        "**Note**: In the example below, you may not get the exact values by multiplying those two numbers, because nltk normalizes each row to have norm of 1. However the relative importance of the terms won't change.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "djkUI6hXtWlr",
        "outputId": "b9c69a53-9a20-4992-fc48-39ee7eba73f3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Using default tokenizer in TfidfVectorizer\n",
        "tfidf = TfidfVectorizer(ngram_range=(1, 1), stop_words=\"english\")\n",
        "features = tfidf.fit_transform(texts)\n",
        "pd.DataFrame(\n",
        "    features.todense(),\n",
        "    columns=tfidf.get_feature_names()\n",
        ")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>10</th>\n",
              "      <th>administration</th>\n",
              "      <th>american</th>\n",
              "      <th>attack</th>\n",
              "      <th>attention</th>\n",
              "      <th>celebrate</th>\n",
              "      <th>companies</th>\n",
              "      <th>connection</th>\n",
              "      <th>degree</th>\n",
              "      <th>direct</th>\n",
              "      <th>donald</th>\n",
              "      <th>drew</th>\n",
              "      <th>economy</th>\n",
              "      <th>foes</th>\n",
              "      <th>fraud</th>\n",
              "      <th>friend</th>\n",
              "      <th>gets</th>\n",
              "      <th>great</th>\n",
              "      <th>growth</th>\n",
              "      <th>increase</th>\n",
              "      <th>january</th>\n",
              "      <th>largest</th>\n",
              "      <th>late</th>\n",
              "      <th>look</th>\n",
              "      <th>lot</th>\n",
              "      <th>market</th>\n",
              "      <th>media</th>\n",
              "      <th>month</th>\n",
              "      <th>occasions</th>\n",
              "      <th>partners</th>\n",
              "      <th>peaked</th>\n",
              "      <th>phony</th>\n",
              "      <th>picassos</th>\n",
              "      <th>plane</th>\n",
              "      <th>platform</th>\n",
              "      <th>policies</th>\n",
              "      <th>political</th>\n",
              "      <th>president</th>\n",
              "      <th>pro</th>\n",
              "      <th>promises</th>\n",
              "      <th>publicly</th>\n",
              "      <th>stocks</th>\n",
              "      <th>strength</th>\n",
              "      <th>takes</th>\n",
              "      <th>traded</th>\n",
              "      <th>trading</th>\n",
              "      <th>trump</th>\n",
              "      <th>twitter</th>\n",
              "      <th>university</th>\n",
              "      <th>using</th>\n",
              "      <th>value</th>\n",
              "      <th>world</th>\n",
              "      <th>worthless</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.27816</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.41724</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.082143</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.082143</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.13908</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.247433</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.41894</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.247433</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.212236</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.424472</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.00000</td>\n",
              "      <td>0.359347</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        10  administration  american  ...    value    world  worthless\n",
              "0  0.13908         0.13908   0.27816  ...  0.13908  0.13908   0.000000\n",
              "1  0.00000         0.00000   0.00000  ...  0.00000  0.00000   0.000000\n",
              "2  0.00000         0.00000   0.00000  ...  0.00000  0.00000   0.359347\n",
              "\n",
              "[3 rows x 53 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ARu2ONJb06Sr"
      },
      "source": [
        "## 4. Text Classification: Alexa reviews\n",
        "\n",
        "we’re going to use a real-world data set: [Amazon Alexa product reviews](https://www.kaggle.com/sid321axn/amazon-alexa-reviews/download).\n",
        "\n",
        "This data set comes as a tab-separated file (.tsv). It has has five columns: `rating`, `date`, `variation`, `verified_reviews`, `feedback`.\n",
        "\n",
        "`rating` denotes the rating each user gave the Alexa (out of 5). `date` indicates the date of the review, and `variation` describes which model the user reviewed. `verified_reviews` contains the text of each review, and `feedback` contains a sentiment label, with 1 denoting positive sentiment (the user liked it) and 0 denoting negative sentiment (the user didn’t).\n",
        "\n",
        "This dataset has consumer reviews of amazon Alexa products like Echos, Echo Dots, Alexa Firesticks etc. What we’re going to do is develop a classification model that looks at the review text and predicts whether a review is positive or negative. Since this data set already includes whether a review is positive or negative in the `feedback` column, we can use those answers to train and test our model. Our goal here is to produce an accurate model that we could then use to process new user reviews and quickly determine whether they were positive or negative."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wzfnR4ua2uLw"
      },
      "source": [
        "### 4.1 Load and prepare data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5rS4hAiA05wQ"
      },
      "source": [
        "# Import additional packages\n",
        "from sklearn.base import TransformerMixin\n",
        "from sklearn.pipeline import Pipeline\n",
        "import string\n",
        "from spacy.lang.en.stop_words import STOP_WORDS\n",
        "from spacy.lang.en import English\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sxrlx4eD0Ygk",
        "outputId": "9a49d0ac-ef78-4543-e5a0-8da95903b46d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load data\n",
        "df = pd.read_csv(\"https://raw.githubusercontent.com/ahmadajal/2019_DM_ML_course/master/6.%20Handling%20Text/data/amazon_alexa.tsv?token=ALM4BCKF2A25WIOVGTL2W327TLN5A\", delimiter=\"\\t\")\n",
        "df.sample(10)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1054</th>\n",
              "      <td>4</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Black  Spot</td>\n",
              "      <td>Worthy successor to the echo dot and right at ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1402</th>\n",
              "      <td>3</td>\n",
              "      <td>31-Jul-18</td>\n",
              "      <td>Black  Show</td>\n",
              "      <td>It seems to work well.  Unfortunately a lot of...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1235</th>\n",
              "      <td>5</td>\n",
              "      <td>26-Jul-18</td>\n",
              "      <td>Black  Spot</td>\n",
              "      <td>It was easy set up.  I use it more than I thou...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2087</th>\n",
              "      <td>5</td>\n",
              "      <td>2-Jul-18</td>\n",
              "      <td>Black  Plus</td>\n",
              "      <td>Love this device</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3133</th>\n",
              "      <td>4</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>I like having more Alexa devices in my house a...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>868</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>BEST father's day gift. Dad joked to my mom th...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2886</th>\n",
              "      <td>3</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>White  Dot</td>\n",
              "      <td>Hard to hear, had to blue tooth it to a better...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>426</th>\n",
              "      <td>5</td>\n",
              "      <td>10-Jul-18</td>\n",
              "      <td>Black</td>\n",
              "      <td>I love the fact, that I can unplug it, and tak...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>846</th>\n",
              "      <td>5</td>\n",
              "      <td>30-Jul-18</td>\n",
              "      <td>Sandstone Fabric</td>\n",
              "      <td>Love it</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>5</td>\n",
              "      <td>29-Jul-18</td>\n",
              "      <td>Heather Gray Fabric</td>\n",
              "      <td>This is a great product!  Set up was easy.  So...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "      rating  ... feedback\n",
              "1054       4  ...        1\n",
              "1402       3  ...        1\n",
              "1235       5  ...        1\n",
              "2087       5  ...        1\n",
              "3133       4  ...        1\n",
              "868        5  ...        1\n",
              "2886       3  ...        1\n",
              "426        5  ...        1\n",
              "846        5  ...        1\n",
              "331        5  ...        1\n",
              "\n",
              "[10 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 25
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WNjV_B2E3nLd",
        "outputId": "fdf2bb3e-a46c-4aa1-efec-f0706a319f6c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3150 entries, 0 to 3149\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype \n",
            "---  ------            --------------  ----- \n",
            " 0   rating            3150 non-null   int64 \n",
            " 1   date              3150 non-null   object\n",
            " 2   variation         3150 non-null   object\n",
            " 3   verified_reviews  3150 non-null   object\n",
            " 4   feedback          3150 non-null   int64 \n",
            "dtypes: int64(2), object(3)\n",
            "memory usage: 123.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6hCSlbq30UT"
      },
      "source": [
        "# Change date to datetime\n",
        "df[\"date\"] = pd.to_datetime(df[\"date\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP8sYT2y34CU",
        "outputId": "0ac8903a-ad0f-44ed-e7c5-dd12a983f761",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "df.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 3150 entries, 0 to 3149\n",
            "Data columns (total 5 columns):\n",
            " #   Column            Non-Null Count  Dtype         \n",
            "---  ------            --------------  -----         \n",
            " 0   rating            3150 non-null   int64         \n",
            " 1   date              3150 non-null   datetime64[ns]\n",
            " 2   variation         3150 non-null   object        \n",
            " 3   verified_reviews  3150 non-null   object        \n",
            " 4   feedback          3150 non-null   int64         \n",
            "dtypes: datetime64[ns](1), int64(2), object(2)\n",
            "memory usage: 123.2+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yK8u3mNw35HC",
        "outputId": "44d4a225-6fa8-4b85-b6af-86441f9fcd3b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Base rate: the data-set is unbalanced!\n",
        "df.feedback.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1    2893\n",
              "0     257\n",
              "Name: feedback, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4d4NPeOq4A0X",
        "outputId": "1657931e-69cc-451a-b4e4-a5b9583d80f8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "round(df.feedback.value_counts()[1] / len(df), 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9184"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHvQECh79AsW",
        "outputId": "3efc6e28-8bc3-4af9-b39e-7fdee6da809e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Convert text into lowercase\n",
        "def clean_text(text):\n",
        "    return text.strip().lower()\n",
        "\n",
        "df[\"verified_reviews\"] = df[\"verified_reviews\"].map(clean_text)\n",
        "\n",
        "df.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>love my echo!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>loved it!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Walnut Finish</td>\n",
              "      <td>sometimes while playing a game, you can answer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>i have had a lot of fun with this thing. my 4 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>music</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating  ... feedback\n",
              "0       5  ...        1\n",
              "1       5  ...        1\n",
              "2       4  ...        1\n",
              "3       5  ...        1\n",
              "4       5  ...        1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2iFd7_hl4eNJ"
      },
      "source": [
        "###### Tokening the Data With spaCy\n",
        "\n",
        "we’ll create a `spacy_tokenizer()` function that accepts a sentence as input and processes the sentence into tokens, performing lemmatization, lowercasing, and removing stop words. \n",
        "\n",
        "__A note from spacy documentation__: spaCy adds a special case for pronouns: all pronouns are lemmatized to the special token `-PRON-`. Unlike verbs and common nouns, there’s no clear base form of a personal pronoun. Should the lemma of “me” be “I”, or should we normalize person as well, giving “it” — or maybe “he”? spaCy’s solution is to introduce a novel symbol, `-PRON-`, which is used as the lemma for all personal pronouns."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hrHhCTFN5ZXw",
        "outputId": "46ea5b54-ee54-466e-a4c0-832d47289df4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create our list of punctuation marks\n",
        "punctuations = string.punctuation\n",
        "\n",
        "punctuations"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCVM6xVQ5fP2",
        "outputId": "18f125ee-4f17-4aa5-87fa-8d9edc67fc72",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Create our list of stopwords\n",
        "stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
        "\n",
        "list(stop_words)[:10]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['so',\n",
              " 'cannot',\n",
              " 'everyone',\n",
              " 'through',\n",
              " 'eight',\n",
              " 'two',\n",
              " 'whereafter',\n",
              " 'an',\n",
              " 'regarding',\n",
              " 'anyone']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-_ALYORw4LN0",
        "outputId": "c5357934-321f-4fa4-d328-7d1413c44245",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Load English language model\n",
        "sp = spacy.load('en_core_web_sm')\n",
        "\n",
        "# Creating our tokenizer function\n",
        "def spacy_tokenizer(sentence):\n",
        "    # Creating our token object, which is used to create documents with linguistic annotations.\n",
        "    mytokens = sp(sentence)\n",
        "\n",
        "    # Lemmatizing each token and converting each token into lowercase\n",
        "    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n",
        "\n",
        "    # Removing stop words and punctuation\n",
        "    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n",
        "\n",
        "    # return preprocessed list of tokens\n",
        "    return mytokens\n",
        "\n",
        "# Example\n",
        "review = df[\"verified_reviews\"].sample()\n",
        "review.values[0]"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'love being able to listen to music easily. still learning all the features available'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dlOwdF_16JV-",
        "outputId": "3e51b562-0ea7-427f-fbdc-39adf73c360e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "spacy_tokenizer(review.values[0])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['love', 'able', 'listen', 'music', 'easily', 'learn', 'feature', 'available']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IZ5Yl8uR9eYv"
      },
      "source": [
        "#### Vectorization Feature Engineering (TF-IDF)\n",
        "\n",
        "We’ll use the TF-IDF (Term Frequency-Inverse Document Frequency) to vectorize the documents. This is a way of representing how important a particular term is in the context of a given document, based on how many times the term appears and how many other documents that same term appears in. The higher the TF-IDF, the more important that term is to that document."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-874sxt8iNM"
      },
      "source": [
        "tfidf_vector = TfidfVectorizer(tokenizer = spacy_tokenizer) # we use the above defined tokenizer"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4jO56WUV9yXn"
      },
      "source": [
        "### 4.2 Classification of the reviews using logistic regression"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L7JkcGTb9ty9",
        "outputId": "34383019-e875-4e9b-e31e-b46e6e581aca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "# Train test split\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "ylabels = df['feedback'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, ylabels, test_size=0.3, random_state=72)\n",
        "\n",
        "X_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "962                                          works great!\n",
              "2041                                           perfect!!!\n",
              "499     now i'm weary about these picking up conversat...\n",
              "3028    small convenient and dependable. this dot is a...\n",
              "2351    very satisfied, easy setup. great product. hig...\n",
              "                              ...                        \n",
              "2885               like having the music where ever i am.\n",
              "3146    listening to music, searching locations, check...\n",
              "1070                 it is very slow compared to the echo\n",
              "1811                                  home entertainment.\n",
              "472     love it even though i’m still trying to figure...\n",
              "Name: verified_reviews, Length: 2205, dtype: object"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6dbSYsF-HRi",
        "outputId": "101887bf-3630-4b02-be6b-b6f3714377b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 225
        }
      },
      "source": [
        "y_train"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "962     1\n",
              "2041    1\n",
              "499     0\n",
              "3028    1\n",
              "2351    1\n",
              "       ..\n",
              "2885    1\n",
              "3146    1\n",
              "1070    0\n",
              "1811    1\n",
              "472     1\n",
              "Name: feedback, Length: 2205, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB8PkfsP-JYi",
        "outputId": "bf2b4384-97eb-4880-843c-8785cce7862c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 416
        }
      },
      "source": [
        "# Define classifier\n",
        "classifier = LogisticRegression(solver=\"lbfgs\")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Pipeline(memory=None,\n",
              "         steps=[('vectorizer',\n",
              "                 TfidfVectorizer(analyzer='word', binary=False,\n",
              "                                 decode_error='strict',\n",
              "                                 dtype=<class 'numpy.float64'>,\n",
              "                                 encoding='utf-8', input='content',\n",
              "                                 lowercase=True, max_df=1.0, max_features=None,\n",
              "                                 min_df=1, ngram_range=(1, 1), norm='l2',\n",
              "                                 preprocessor=None, smooth_idf=True,\n",
              "                                 stop_words=None, strip_accents=None,\n",
              "                                 sublinear_tf=False,\n",
              "                                 token_patt...,\n",
              "                                 tokenizer=<function spacy_tokenizer at 0x7fb4e853d158>,\n",
              "                                 use_idf=True, vocabulary=None)),\n",
              "                ('classifier',\n",
              "                 LogisticRegression(C=1.0, class_weight=None, dual=False,\n",
              "                                    fit_intercept=True, intercept_scaling=1,\n",
              "                                    l1_ratio=None, max_iter=100,\n",
              "                                    multi_class='auto', n_jobs=None,\n",
              "                                    penalty='l2', random_state=None,\n",
              "                                    solver='lbfgs', tol=0.0001, verbose=0,\n",
              "                                    warm_start=False))],\n",
              "         verbose=False)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e07kHgHb-hC2"
      },
      "source": [
        "# Evaluate the model\n",
        "def evaluate(true, pred):\n",
        "    precision = precision_score(true, pred)\n",
        "    recall = recall_score(true, pred)\n",
        "    f1 = f1_score(true, pred)\n",
        "    print(f\"CONFUSION MATRIX:\\n{confusion_matrix(true, pred)}\")\n",
        "    print(f\"ACCURACY SCORE:\\n{accuracy_score(true, pred):.4f}\")\n",
        "    print(f\"CLASSIFICATION REPORT:\\n\\tPrecision: {precision:.4f}\\n\\tRecall: {recall:.4f}\\n\\tF1_Score: {f1:.4f}\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PjJWqHp_HvQ",
        "outputId": "e05d5f1e-a6e9-425f-dd54-a6bdc3b0744a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 173
        }
      },
      "source": [
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "evaluate(y_test, y_pred)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  1  63]\n",
            " [  0 881]]\n",
            "ACCURACY SCORE:\n",
            "0.9333\n",
            "CLASSIFICATION REPORT:\n",
            "\tPrecision: 0.9333\n",
            "\tRecall: 1.0000\n",
            "\tF1_Score: 0.9655\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRrITD4y_nbb"
      },
      "source": [
        "Our model correctly identified a comment’s sentiment 93.6% of the time. This is greater than the base rate, so we are fine. When it predicted a review was positive, that review was actually positive 93.2% of the time. When handed a positive review, our model identified it as positive 100% of the time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L9D5xSc5_y9i",
        "outputId": "cbaa9f17-3646-4dd6-cebe-58b5c77d1186",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 202
        }
      },
      "source": [
        "# BONUS: predict the rating\n",
        "df.sample()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rating</th>\n",
              "      <th>date</th>\n",
              "      <th>variation</th>\n",
              "      <th>verified_reviews</th>\n",
              "      <th>feedback</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>love my echo!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>loved it!</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Walnut Finish</td>\n",
              "      <td>sometimes while playing a game, you can answer...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>i have had a lot of fun with this thing. my 4 ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5</td>\n",
              "      <td>2018-07-31</td>\n",
              "      <td>Charcoal Fabric</td>\n",
              "      <td>music</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rating  ... feedback\n",
              "0       5  ...        1\n",
              "1       5  ...        1\n",
              "2       4  ...        1\n",
              "3       5  ...        1\n",
              "4       5  ...        1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CiyyP7T6Bykf",
        "outputId": "9127a2c5-5a76-4462-8e4f-469518eb0caf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "df.rating.value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5    2286\n",
              "4     455\n",
              "1     161\n",
              "3     152\n",
              "2      96\n",
              "Name: rating, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BNNejlJgB2aY",
        "outputId": "85c3e0ad-6936-401e-ddfd-0dd2bf6a145d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "# Base rate\n",
        "round(df.rating.value_counts()[5] / len(df), 4)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.7257"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 46
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ZE1JqorB9Lb",
        "outputId": "c3cf447c-c210-41d0-8de3-70552a1ea63c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# Train test split\n",
        "X = df['verified_reviews'] # the features we want to analyze\n",
        "y = df['rating'] # the labels, or answers, we want to test against\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=72)\n",
        "\n",
        "# Define classifier\n",
        "classifier = LogisticRegression(solver=\"lbfgs\")\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[  4   0   0   0  39]\n",
            " [  0   0   0   1  20]\n",
            " [  0   0   2   6  45]\n",
            " [  1   0   0   8 128]\n",
            " [  0   0   0   5 686]]\n",
            "ACCURACY SCORE:\n",
            "0.7407\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EfE1iXi0Eeuz",
        "outputId": "d5ba7df2-011c-4feb-8d97-6b4ec0ddf42e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 156
        }
      },
      "source": [
        "# BONUS 2: use random forest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Define classifier\n",
        "classifier = RandomForestClassifier()\n",
        "\n",
        "# Create pipeline\n",
        "pipe = Pipeline([('vectorizer', tfidf_vector),\n",
        "                 ('classifier', classifier)])\n",
        "\n",
        "# Generate Model on training set\n",
        "pipe.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = pipe.predict(X_test)\n",
        "\n",
        "# Evaluation\n",
        "print(f\"CONFUSION MATRIX:\\n{confusion_matrix(y_test, y_pred)}\")\n",
        "print(f\"ACCURACY SCORE:\\n{accuracy_score(y_test, y_pred):.4f}\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "CONFUSION MATRIX:\n",
            "[[ 14   0   0   0  29]\n",
            " [  2   8   0   0  11]\n",
            " [  0   1  19   2  31]\n",
            " [  1   0   0  42  94]\n",
            " [  0   0   0   6 685]]\n",
            "ACCURACY SCORE:\n",
            "0.8127\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yFt-MD98E1E4"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}