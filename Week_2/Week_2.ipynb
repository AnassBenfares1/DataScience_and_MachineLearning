{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sPgLYyUIzy-L"
      },
      "source": [
        "# Python Notebooks\n",
        "\n",
        "## Goals\n",
        "The goal of this notebook is to briefly introduce you to the main tools we will use in this practical seminar: Python code and markdown Notebooks.  The focus will be on using markdown to create interactive notebooks, which you can share with others.\n",
        "\n",
        "## Getting started\n",
        "\n",
        "**Notebooks** represent an interactive environment for writing and executing code, while at the same time also documenting the code with rich text. Here we review some elementary ways of creating your notebooks.\n",
        "\n",
        "We will be using **Google Colab**, which is a free notebook environment, that does not require any installations and uses Google servers to execute the code. You can simply run it in your browser, after you create a free Google account (if not yet done).\n",
        "\n",
        "To open a new notebook in Google Colab click [here](https://colab.research.google.com/) and then click on the \"New Notebook\" at the bottom right.\n",
        "\n",
        "You can open this notebook in Colab by clicking on the Open in Colab badge on top of the file.\n",
        "\n",
        "A notebook consists of two kinds of cells:\n",
        "* cellls that contain executable **code** and\n",
        "* cells that contain **markdown (text)**.\n",
        "\n",
        "Here we'll provide a more detailed introduction to the text cells, while the following seminars we'll focus more on the code cells.\n",
        "\n",
        "Markdown syntax is very similar to HTML, but it is more simplified. Here are some additional introductory ressources:\n",
        "- Try markdown on your own in [Google Colab](https://colab.research.google.com/notebooks/markdown_guide.ipynb)\n",
        "- You can also find a more detailed and [lengthy introduction](https://about.gitlab.com/handbook/product/technical-writing/markdown-guide/).\n",
        "\n",
        "### Switching between Code and Text/Markdown\n",
        "\n",
        "- If you want to include markdown in a code cell, you can convert it to a text cell by pressing `control M M` (that is hold down the `control` key and press two times the letter `M`).\n",
        "- You can convert a text cell into a code cell using the shortcut `control M Y`."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hKSzCk0D4WrP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kXvAgbBGzy-W"
      },
      "source": [
        "<div class=\"alert alert-block alert-info\">\n",
        "\n",
        "\n",
        "<h2>Shortcuts</h2>\n",
        "\n",
        "It's good to learn the following shortcuts:\n",
        "\n",
        "\n",
        "- Execute a cell: shift-enter on Mac (ctrl-enter for Windows)\n",
        "- Saving the notebook: command s on a Mac, (ctrl-s for Windows)\n",
        "- Change Cell types: ctrl M Y (code), ctrl M M (markdown)\n",
        "\n",
        "<h2>Saving & downloading your notebook</h2>\n",
        "\n",
        "You can save your notebook in your Google Drive using the Colab Menu > File > Save a copy in Drive (Google Drive), or directly in GitHub.\n",
        "\n",
        "You can also download the notebook using File>Download>  .ipynb (notebook format).\n",
        "\n",
        "</div>\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Basic operations with Pandas\n",
        "\n",
        "<img width=\"200\" src='https://upload.wikimedia.org/wikipedia/commons/e/ed/Pandas_logo.svg'>\n",
        "\n",
        "##Data cleaning\n",
        "### How Does Data Get Dirty?\n",
        "- Missing data.\n",
        "- Inconsistent data.\n",
        "- Duplicate data.\n",
        "\n",
        "This is just to name a few things that can go wrong. There is an endless list of ways that data can end up very messy.\n",
        "Sometimes there are insufficient validation checks when the data is entered in the first place.\n",
        "If you have form fields with users entering data in any format they want with no guidelines or form validation checks in place to enforce conforming to a certain format, then users will input however they see fit.\n",
        "\n",
        "There could be an input field for the state (U.S.) and you have some data that is the two-character abbreviation, NY and then others have New York, then there are potential misspellings and typos, etc.\n",
        "\n",
        "Data can also become corrupted during transmission or in storage."
      ],
      "metadata": {
        "id": "4bVDcyJHHFjh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "df = pd.read_csv(\"userRecords.csv\")\n"
      ],
      "metadata": {
        "id": "abwjDTZRHvzn",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 374
        },
        "outputId": "c788ae90-7678-44c4-bef0-b17762a60854"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-0fb5baea67b1>\u001b[0m in \u001b[0;36m<cell line: 4>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"userRecords.csv\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    209\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m                     \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnew_arg_name\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_arg_value\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfind_stack_level\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    330\u001b[0m                 )\n\u001b[0;32m--> 331\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    332\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[0;31m# error: \"Callable[[VarArg(Any), KwArg(Any)], Any]\" has no\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    948\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    949\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 950\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    951\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    952\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    603\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 605\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    606\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    607\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1440\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1441\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1442\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1443\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1444\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1733\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1734\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1735\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1736\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1737\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    854\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    855\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 856\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    857\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'userRecords.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Shape of the data\n",
        "\n",
        "Let's first check how many rows and columns (features) are in this dataset"
      ],
      "metadata": {
        "id": "Fl_HmWwtIBDJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.shape"
      ],
      "metadata": {
        "id": "uICj1b0GIEKG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Check out the first few rows\n",
        "You can look at the first few rows by calling `head()` on the dataframe."
      ],
      "metadata": {
        "id": "JT6l8O-2ISqh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "xce5aB8SIOeu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### List the column/feature names"
      ],
      "metadata": {
        "id": "3TgOsG-pI8Yo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.columns"
      ],
      "metadata": {
        "id": "KPRr2ogtI-3B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Duplicates\n",
        "You can check if there are duplicates in the dataset."
      ],
      "metadata": {
        "id": "e1XYn9tvJHbL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.duplicated()"
      ],
      "metadata": {
        "id": "Ve6cJ5hbJIrQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "This shows which are the duplicated rows if any. None are duplicated here."
      ],
      "metadata": {
        "id": "tebuO3lxJP-_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.loc[df.duplicated()=='True', :]"
      ],
      "metadata": {
        "id": "5UPbuQn5JQnQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can check if the same value appears in a single column more than once in the dataset. Are there multiple users with the same birthday?"
      ],
      "metadata": {
        "id": "pLJGJqwPT8B9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.birthday.duplicated()"
      ],
      "metadata": {
        "id": "vNPN6bqVUjDq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also filter the table so that only entries with given characteristics are maintained, for example users from Switzerland (CH)"
      ],
      "metadata": {
        "id": "-Akjuv9hBsPg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['country']=='CH'].head()"
      ],
      "metadata": {
        "id": "VxQYqj_YB-TQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also get the list of people who have their birthday the same day as another person in the dataset"
      ],
      "metadata": {
        "id": "ZtNDR-qOzFfQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df[df.birthday.duplicated()].head()"
      ],
      "metadata": {
        "id": "R7xDb_JPyD0x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also show all users who have their birthday on the same day by checking who has their birthday on one of the duplicated dates."
      ],
      "metadata": {
        "id": "FdIP5xyv2Qfs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "duplicate_dates = df[df.birthday.duplicated()]['birthday'].to_list()\n",
        "\n",
        "df[df['birthday'].isin(duplicate_dates)].sort_values('birthday').head()"
      ],
      "metadata": {
        "id": "JDXzbAGPzVXy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing/Null values\n",
        "You can call `isnull()` and `sum()` to get a count of how many null values are there in each column."
      ],
      "metadata": {
        "id": "OH0Rp5xb3OH9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.isnull().sum()"
      ],
      "metadata": {
        "id": "27y9FBJG3MuR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Types of the Features\n",
        "It is important that the data values in each column have the correct data type. For example, you can expect a column containing numbers to be in numeric format, but sometimes you will find string values in it. In such a case, when you do numeric calculations on that column you might get unexpected results.\n",
        "\n",
        "The property `dtypes` will show you the data types for each column in the dataframe."
      ],
      "metadata": {
        "id": "LGxOw60564XO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dtypes"
      ],
      "metadata": {
        "id": "oy_gOccG656c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can notice that the column `issue_date` is in integer format (`int64`), whereas it should be in datetime format. In Pandas you can convert a column to datetime format using `to_datetime` method."
      ],
      "metadata": {
        "id": "6id3Cc5A857W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df['birthday'].head()"
      ],
      "metadata": {
        "id": "G5_uJ8Ua86nW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# if you do not provide the format, the function may infer the wrong format\n",
        "pd.to_datetime(df['birthday']).head()"
      ],
      "metadata": {
        "id": "_f5ix4CB8-sn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['birthday'] = pd.to_datetime(df['birthday'], format=\"%d-%m-%Y\")\n",
        "df['birthday'].head()"
      ],
      "metadata": {
        "id": "ZdIwEsiX9kZj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dropping the columns you are not interested in\n",
        "Let's say we are only interested in users last names, their first name and their origin. So we are going to drop the rest of the columns.\n",
        "\n",
        "Note that many operations in Pandas can be done on a copy of the data or in-place (affecting the original data frame). You can use the parameter `inplace=True`.\n",
        "\n",
        "> Also note that in Pandas, axis 0 represents the rows while axis 1 represents the columns."
      ],
      "metadata": {
        "id": "F2AVjpBZ-ErR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df.dropna().head()"
      ],
      "metadata": {
        "id": "k1g_M2pLlCsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_delete = ['UserID', 'birthday']\n",
        "df.drop(columns_to_delete, inplace=True, axis=1)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "bGLiGurn-cRY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also select the columns to keep instead of selecting the columns to delete"
      ],
      "metadata": {
        "id": "ft28xhTS_2M_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "columns_to_keep = ['first_name', 'last_name', 'country']\n",
        "df = df[columns_to_keep]\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nYgk-ozd_7IN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the help of the value_counts() function you can see how many people in the dataset are from each country.\n",
        "You can also use it to find how many maried people we have per country."
      ],
      "metadata": {
        "id": "3omP7MUKJGIk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"userRecords.csv\")\n",
        "df[\"country\"].value_counts().sort_values(ascending = False).head()"
      ],
      "metadata": {
        "id": "RjlAyKbuJibj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df[df['marital_status'] == 1]['country'].value_counts().head()"
      ],
      "metadata": {
        "id": "FfRvP4hvaC8Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can also use the group by method to aggregate data depending on one category.\n",
        "\n",
        "We can use it to find the the average number of kids per country"
      ],
      "metadata": {
        "id": "dZfWwEk2dQyk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('userRecords.csv')\n",
        "df= df[['country', 'number_of_kids']]\n",
        "df = df.dropna()\n",
        "\n",
        "df = df.groupby('country').mean().sort_values('number_of_kids', ascending=False)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "KqiRH021JqbF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Your turn !\n"
      ],
      "metadata": {
        "id": "ZZxIthH6NYQR"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now a small exercice so that you can practice what we have learned so far.\n",
        "Based on this [new dataset](https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Week_2/movie_data.csv), answer these questions:\n",
        "\n",
        "\n",
        "*   What is the shape of the dataset?\n",
        "*   What are the names of the columns ?\n",
        "*   Are there any duplicates ? If yes, which ones ?\n",
        "*   Are there any missing values ? If yes, which ones ?\n",
        "*   How many movies have a rating of 5 ? Create a list with the name of the movies\n",
        "\n"
      ],
      "metadata": {
        "id": "c_U_te7vNcnb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "8lS4kDcqNbLg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Operations with multiple datasets using pandas\n",
        "\n",
        "Sometimes data sets may come from different sources. In those cases you may need to merge datasets together.\n",
        "\n",
        "Let's assume two datasets, user data and flight records.\n",
        "\n",
        "The two data sets can be linked through the \"user_id\" column."
      ],
      "metadata": {
        "id": "KkWJBx21Cq41"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#check the dataset with user data\n",
        "user_data = pd.read_csv('userRecords.csv')\n",
        "user_data = user_data[['UserID', 'first_name', 'last_name']]\n",
        "user_data.head(2)"
      ],
      "metadata": {
        "id": "EL7JbTKQCqUR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the dataset with the flight records\n",
        "flight_records = pd.read_csv('flightRecords.csv')\n",
        "flight_records.head(2)"
      ],
      "metadata": {
        "id": "C8XXKKWVGa5_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Merging](https://pandas.pydata.org/docs/user_guide/merging.html) the two datasets:"
      ],
      "metadata": {
        "id": "Mp1SB0TPKoKM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = pd.merge(flight_records, user_data, left_on=\"userID\", right_on='UserID', how=\"left\", sort=False)\n",
        "combined_dataset = combined_dataset.sort_values('date')\n",
        "\n",
        "combined_dataset.head(2)"
      ],
      "metadata": {
        "id": "SJbpo03uHBcW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we can find who traveled on a given date, for example on the 1st of december 2022"
      ],
      "metadata": {
        "id": "EmNRTgx2LnyW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset[combined_dataset['date']=='01-12-2022']"
      ],
      "metadata": {
        "id": "38tF-z8HLndV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We can also find the travels of all people who's first name is \"Nicolas\""
      ],
      "metadata": {
        "id": "XYOqxVGCM972"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset[combined_dataset['first_name']=='Nicolas'].head(3)"
      ],
      "metadata": {
        "id": "7QDP9fQUM-Wo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Your turn !"
      ],
      "metadata": {
        "id": "s4fEuIS4VCHP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "With the [netflix dataset](https://raw.githubusercontent.com/michalis0/DataScience_and_MachineLearning/master/Week_2/netflix_data.csv), merge the two dataset on the movie's title and give the movie that has the best Netflic Score and a IMDB score over 4.5."
      ],
      "metadata": {
        "id": "3ZZjkkedaePG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XB4Xva2xaa_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#[Data visualisation ](https://pandas.pydata.org/docs/user_guide/visualization.html)\n",
        "\n",
        "Data can be hard to comprehend on it's own,\n",
        "Sometimes visualization tools can be helpfull to get a better sense of it."
      ],
      "metadata": {
        "id": "6nh27RK8cMJZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's first make a pie chart to visualise the sum of the children born in the top 5 most fertile countries"
      ],
      "metadata": {
        "id": "5mX8w6H7c6ro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('userRecords.csv')\n",
        "df= df[['country', 'number_of_kids']]\n",
        "df = df.dropna()\n",
        "df = df.groupby('country').sum().sort_values('number_of_kids', ascending=False)\n",
        "df = df[0:6]\n",
        "series = pd.Series(df['number_of_kids'].to_list(), index=df.index.to_list())\n",
        "\n",
        "\n",
        "series.plot.pie(autopct=\"%.2f\",figsize=(6, 6));"
      ],
      "metadata": {
        "id": "5Muyk-xMc7h6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Boxplots are also usefull to compare data\n",
        "\n",
        "For the following exercise we will compare mobile data consumption per country"
      ],
      "metadata": {
        "id": "WBUy4QHHiWuO"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lets first combined the phone logs dataset with the antenna info dataset in order to link each antena to a country"
      ],
      "metadata": {
        "id": "uiKyLYwHcCJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "phone_logs = pd.read_csv(\"all_phone_logs.csv\")\n",
        "phone_logs.head()"
      ],
      "metadata": {
        "id": "QDXQ5-86cR61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "antenna_info = pd.read_csv(\"antena_info.csv\")\n",
        "antenna_info.head()"
      ],
      "metadata": {
        "id": "iaAAS_Ubccle"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset = pd.merge(phone_logs, antenna_info, left_on=\"antena_id\", right_on='antena_id', how=\"left\", sort=False)\n",
        "combined_dataset = combined_dataset.sort_values('country')\n",
        "\n",
        "combined_dataset.head(2)"
      ],
      "metadata": {
        "id": "2xU4gw37cOX6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consumption_per_country = combined_dataset[['dandwith_used', 'country']]\n",
        "consumption_per_country = consumption_per_country.groupby('country').sum()\n",
        "consumption_per_country.head(5)"
      ],
      "metadata": {
        "id": "fBRaUIuWdrR_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "consumption_per_country.plot.box()"
      ],
      "metadata": {
        "id": "JbugOuODe2rd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Let's now find the consumption per country per year, and then display it in boxplots"
      ],
      "metadata": {
        "id": "IcGc10KCfFL7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "combined_dataset['date_and_time'] = pd.to_datetime(combined_dataset['date_and_time']).to_list()\n",
        "combined_dataset['year'] = list(map(lambda x: x.year,combined_dataset['date_and_time'].to_list()))\n",
        "\n",
        "\n",
        "consumption_per_country_per_year = combined_dataset[['dandwith_used', 'country', 'year']]\n",
        "consumption_per_country_per_year = consumption_per_country_per_year.groupby(['country', 'year']).sum()\n",
        "consumption_per_country_per_year.boxplot(by = 'year')"
      ],
      "metadata": {
        "id": "fQDGqcsffcIe"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}